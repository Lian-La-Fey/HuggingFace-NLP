{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Processing the data\n\nÖnceki bölümdeki örnekle devam edersek, PyTorch'ta bir yığın üzerinde bir dizi sınıflandırıcıyı nasıl eğiteceğimiz aşağıda açıklanmıştır:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Same as before\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"This course is amazing!\",\n]\nbatch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n\n# This is new\nbatch[\"labels\"] = torch.tensor([1, 1])\n\noptimizer = torch.optim.AdamW(model.parameters())\nloss = model(**batch).loss\nloss.backward()\noptimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:19.350099Z","iopub.execute_input":"2024-08-07T06:23:19.350490Z","iopub.status.idle":"2024-08-07T06:23:31.712113Z","shell.execute_reply.started":"2024-08-07T06:23:19.350457Z","shell.execute_reply":"2024-08-07T06:23:31.710999Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9607a88385554b139b6689c2ab6d9692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8902a52ff37b4a2a855f99c44c9db720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c43e327f9924f18a30fc1a6c00e5552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4caec8a74c542aea4cd12802139d865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4d40ffee2148c38d0d41b4c8e004e9"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Elbette, modeli sadece iki cümle üzerinde eğitmek çok iyi sonuçlar vermeyecektir. Daha iyi sonuçlar elde etmek için daha büyük bir veri kümesi hazırlamanız gerekecektir.\n\nBu bölümde örnek olarak William B. Dolan ve Chris Brockett tarafından bir makalede tanıtılan MRPC (Microsoft Research Paraphrase Corpus) veri kümesini kullanacağız. Bu veri kümesi 5.801 cümle çiftinden oluşmaktadır ve bu cümlelerin açımlama olup olmadığını (yani her iki cümlenin de aynı anlama gelip gelmediğini) gösteren bir etikete sahiptir. Bu bölüm için bu veri kümesini seçtik çünkü küçük bir veri kümesidir, bu nedenle üzerinde eğitim denemesi yapmak kolaydır.","metadata":{}},{"cell_type":"markdown","source":"## Loading a dataset from the Hub\n\nHub sadece modeller içermez; aynı zamanda birçok farklı dilde birden fazla veri kümesine sahiptir. Veri setlerine [buradan](https://huggingface.co/datasets) göz atabilirsiniz ve bu bölümden geçtikten sonra yeni bir veri seti yüklemeyi ve işlemeyi denemenizi öneririz (buradaki genel belgelere bakın). Ancak şimdilik MRPC veri setine odaklanalım! Bu, 10 farklı metin sınıflandırma görevinde makine öğrenimi modellerinin performansını ölçmek için kullanılan akademik bir ölçüt olan GLUE ölçütünü oluşturan 10 veri kümesinden biridir.\n\nDatasets kütüphanesi, Hub'a bir veri kümesi indirmek ve önbelleğe almak için çok basit bir komut sağlar. MRPC veri kümesini şu şekilde indirebiliriz:","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:31.715007Z","iopub.execute_input":"2024-08-07T06:23:31.715853Z","iopub.status.idle":"2024-08-07T06:23:39.883908Z","shell.execute_reply.started":"2024-08-07T06:23:31.715806Z","shell.execute_reply":"2024-08-07T06:23:39.882574Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83954140848e40d19d5eff90ea2fcf1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9b027159de4801aceec15849ef8dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9131af523e644868da439f0318a788f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6602f2f842848a380d66c02e02aef04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25a9ec02869a446fa708a90bb770b97e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74835a58ef9a4fdb88b70cbaceaf0227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1344c03a6554cdaa5dc5ab7fb5ee362"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Gördüğünüz gibi, eğitim kümesi, doğrulama kümesi ve test kümesini içeren bir DatasetDict nesnesi elde ediyoruz. Bunların her biri birkaç sütun (cümle1, cümle2, etiket ve idx) ve her kümedeki öğe sayısı olan değişken sayıda satır içerir (yani, eğitim kümesinde 3.668, doğrulama kümesinde 408 ve test kümesinde 1.725 cümle çifti vardır).\n\nBu komut veri kümesini indirir ve varsayılan olarak ~/.cache/huggingface/datasets içinde önbelleğe alır. HF_HOME ortam değişkenini ayarlayarak önbellek klasörünüzü özelleştirebileceğinizi Bölüm 2'den hatırlayın.\n\nraw_datasets nesnemizdeki her bir cümle çiftine bir sözlükte olduğu gibi indeksleme yaparak erişebiliriz:","metadata":{}},{"cell_type":"code","source":"raw_train_dataset = raw_datasets[\"train\"]\nraw_train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:39.885310Z","iopub.execute_input":"2024-08-07T06:23:39.885844Z","iopub.status.idle":"2024-08-07T06:23:39.895268Z","shell.execute_reply.started":"2024-08-07T06:23:39.885813Z","shell.execute_reply":"2024-08-07T06:23:39.894003Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n 'label': 1,\n 'idx': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"Etiketlerin zaten tamsayı olduğunu görebiliyoruz, bu nedenle burada herhangi bir ön işlem yapmamız gerekmeyecek. Hangi tamsayının hangi etikete karşılık geldiğini bilmek için raw_train_dataset'imizin özelliklerini inceleyebiliriz. Bu bize her sütunun türünü söyleyecektir:","metadata":{}},{"cell_type":"code","source":"raw_train_dataset.features","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:39.896895Z","iopub.execute_input":"2024-08-07T06:23:39.897368Z","iopub.status.idle":"2024-08-07T06:23:39.914886Z","shell.execute_reply.started":"2024-08-07T06:23:39.897329Z","shell.execute_reply":"2024-08-07T06:23:39.913650Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'idx': Value(dtype='int32', id=None)}"},"metadata":{}}]},{"cell_type":"markdown","source":"Perde arkasında, label ClassLabel türündedir ve tamsayıların label adıyla eşleştirilmesi names klasöründe saklanır. 0 not_equivalent'e karşılık gelir ve 1 equivalent'e karşılık gelir.","metadata":{}},{"cell_type":"code","source":"raw_train_dataset[15], raw_datasets[\"validation\"][87]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:39.918193Z","iopub.execute_input":"2024-08-07T06:23:39.918601Z","iopub.status.idle":"2024-08-07T06:23:39.932186Z","shell.execute_reply.started":"2024-08-07T06:23:39.918566Z","shell.execute_reply":"2024-08-07T06:23:39.930781Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"({'sentence1': 'Rudder was most recently senior vice president for the Developer & Platform Evangelism Business .',\n  'sentence2': 'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .',\n  'label': 0,\n  'idx': 16},\n {'sentence1': 'However , EPA officials would not confirm the 20 percent figure .',\n  'sentence2': 'Only in the past few weeks have officials settled on the 20 percent figure .',\n  'label': 0,\n  'idx': 812})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing a dataset\n\nVeri kümesini ön işleme tabi tutmak için metni modelin anlamlandırabileceği sayılara dönüştürmemiz gerekir. Önceki bölümde gördüğünüz gibi, bu bir tokenizer ile yapılır. Tokenizer bir cümle veya cümleler listesi ile besleyebiliriz, böylece her çiftin tüm ilk cümlelerini ve tüm ikinci cümlelerini aşağıdaki gibi doğrudan belirteçleyebiliriz:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\ntokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:39.933509Z","iopub.execute_input":"2024-08-07T06:23:39.933979Z","iopub.status.idle":"2024-08-07T06:23:40.521429Z","shell.execute_reply.started":"2024-08-07T06:23:39.933912Z","shell.execute_reply":"2024-08-07T06:23:40.520201Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Ancak, iki diziyi modele aktararak iki cümlenin paraphrase olup olmadığına dair bir tahmin elde edemeyiz. İki diziyi bir çift olarak ele almamız ve uygun ön işlemeyi uygulamamız gerekir. Neyse ki tokenizer da bir çift diziyi alıp BERT modelimizin beklediği şekilde hazırlayabilir:","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\ninputs","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:40.522971Z","iopub.execute_input":"2024-08-07T06:23:40.523367Z","iopub.status.idle":"2024-08-07T06:23:40.531470Z","shell.execute_reply.started":"2024-08-07T06:23:40.523334Z","shell.execute_reply":"2024-08-07T06:23:40.530262Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"markdown","source":"Bölüm 2'de input_ids ve attention_mask anahtarlarından bahsetmiştik, ancak token_type_ids hakkında konuşmayı ertelemiştik. Bu örnekte, modele girdinin hangi kısmının ilk cümle, hangisinin ikinci cümle olduğunu söyleyen şey budur.","metadata":{}},{"cell_type":"markdown","source":"Eğer input_ids içindeki ID'leri kelimelere geri dönüştürürsek:","metadata":{}},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:40.533170Z","iopub.execute_input":"2024-08-07T06:23:40.533559Z","iopub.status.idle":"2024-08-07T06:23:40.545669Z","shell.execute_reply.started":"2024-08-07T06:23:40.533529Z","shell.execute_reply":"2024-08-07T06:23:40.544376Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'this',\n 'is',\n 'the',\n 'first',\n 'sentence',\n '.',\n '[SEP]',\n 'this',\n 'is',\n 'the',\n 'second',\n 'one',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"markdown","source":"Dolayısıyla, modelin iki cümle olduğunda girdilerin [CLS] cümle1 [SEP] cümle2 [SEP] biçiminde olmasını beklediğini görüyoruz. Bunu token_type_ids ile hizalamak bize şunu verir:","metadata":{}},{"cell_type":"code","source":"['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:40.547465Z","iopub.execute_input":"2024-08-07T06:23:40.548008Z","iopub.status.idle":"2024-08-07T06:23:40.561132Z","shell.execute_reply.started":"2024-08-07T06:23:40.547950Z","shell.execute_reply":"2024-08-07T06:23:40.559620Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"},"metadata":{}}]},{"cell_type":"markdown","source":"Gördüğünüz gibi, girdinin [CLS] cümle1 [SEP]'e karşılık gelen kısımlarının tümü 0 belirteç türü kimliğine sahipken, cümle2 [SEP]'e karşılık gelen diğer kısımların tümü 1 belirteç türü kimliğine sahiptir.\n\nFarklı bir kontrol noktası seçerseniz, token_type_id'lerin tokenize edilmiş girdilerinizde olması gerekmeyeceğini unutmayın (örneğin, bir DistilBERT modeli kullanıyorsanız bunlar döndürülmez). Bunlar yalnızca model bunlarla ne yapacağını bildiğinde döndürülür, çünkü ön eğitim sırasında bunları görmüştür.\n\nBurada BERT, belirteç türü kimlikleriyle ön eğitime tabi tutulmuştur ve Bölüm 1'de bahsettiğimiz maskeli dil modelleme hedefinin yanı sıra, sonraki cümle tahmini adı verilen ek bir hedefe sahiptir. Bu görevdeki amaç, cümle çiftleri arasındaki ilişkiyi modellemektir.\n\nSonraki cümle tahmini ile modele cümle çiftleri (rastgele maskelenmiş belirteçlerle) verilir ve ikinci cümlenin ilkini takip edip etmediğini tahmin etmesi istenir. Görevi önemsiz hale getirmemek için, cümleler zamanın yarısında çıkarıldıkları orijinal belgede birbirini takip eder ve diğer yarısında iki cümle iki farklı belgeden gelir.\n\nGenel olarak, tokenize edilmiş girdilerinizde token_type_ids olup olmadığı konusunda endişelenmenize gerek yoktur: tokenizer ve model için aynı kontrol noktasını kullandığınız sürece, tokenizer modeline ne sağlayacağını bildiği için her şey yolunda gidecektir.\n\nArtık tokenizer'ımızın bir çift cümleyle nasıl başa çıkabildiğini gördüğümüze göre, onu tüm veri setimizi tokenize etmek için kullanabiliriz: önceki bölümde olduğu gibi, tokenizer'a ilk cümlelerin listesini ve ardından ikinci cümlelerin listesini vererek cümle çiftlerinin bir listesini besleyebiliriz. Bu aynı zamanda Bölüm 2'de gördüğümüz dolgu ve kesme seçenekleriyle de uyumludur. Yani, eğitim veri kümesini önceden işlemenin bir yolu şudur:","metadata":{}},{"cell_type":"code","source":"tokenized_dataset = tokenizer(\n    raw_datasets[\"train\"][\"sentence1\"],\n    raw_datasets[\"train\"][\"sentence2\"],\n    padding=True,\n    truncation=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:40.563204Z","iopub.execute_input":"2024-08-07T06:23:40.563684Z","iopub.status.idle":"2024-08-07T06:23:41.218427Z","shell.execute_reply.started":"2024-08-07T06:23:40.563643Z","shell.execute_reply":"2024-08-07T06:23:41.216908Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Bu iyi çalışır, ancak bir sözlük döndürme dezavantajına sahiptir (anahtarlarımız, input_ids, attention_mask ve token_type_ids ve listelerin listeleri olan değerlerle). Ayrıca, yalnızca tokenizasyon sırasında tüm veri kümenizi depolamak için yeterli RAM'iniz varsa çalışacaktır (oysa Datasets kütüphanesindeki veri kümeleri diskte depolanan Apache Arrow dosyalarıdır, bu nedenle yalnızca istediğiniz örnekleri bellekte yüklü tutarsınız).\n\nVerileri bir veri kümesi olarak tutmak için Dataset.map() yöntemini kullanacağız. Bu aynı zamanda, sadece tokenizasyondan daha fazla ön işleme ihtiyacımız olursa bize ekstra esneklik sağlar. map() metodu, veri kümesinin her bir elemanına bir fonksiyon uygulayarak çalışır, bu yüzden girdilerimizi tokenize eden bir fonksiyon tanımlayalım:","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:41.220102Z","iopub.execute_input":"2024-08-07T06:23:41.220452Z","iopub.status.idle":"2024-08-07T06:23:41.226814Z","shell.execute_reply.started":"2024-08-07T06:23:41.220423Z","shell.execute_reply":"2024-08-07T06:23:41.225349Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Bu fonksiyon bir dictionary alır (veri kümemizin öğeleri gibi) ve input_ids, attention_mask ve token_type_ids anahtarlarını içeren yeni bir sözlük döndürür. Daha önce görüldüğü gibi, tokenizer cümle çiftleri listeleri üzerinde çalıştığından, örnek sözlük birkaç örnek içeriyorsa (her anahtar bir cümle listesi olarak) da çalıştığını unutmayın. Bu, map() çağrımızda batched=True seçeneğini kullanmamıza olanak tanıyacak ve bu da tokenizasyonu büyük ölçüde hızlandıracaktır. Tokenizer,  Tokenizers kütüphanesinden Rust ile yazılmış bir tokenizer tarafından desteklenmektedir. Bu tokenizer çok hızlı olabilir, ancak yalnızca aynı anda çok sayıda girdi verirsek.\n\nŞimdilik tokenizasyon fonksiyonumuzda padding argümanını dışarıda bıraktığımızı unutmayın. Bunun nedeni, tüm örnekleri maksimum uzunluğa kadar doldurmanın verimli olmamasıdır: bir yığın oluştururken örnekleri doldurmak daha iyidir, çünkü o zaman tüm veri kümesindeki maksimum uzunluğa değil, yalnızca o yığındaki maksimum uzunluğa kadar doldurmamız gerekir. Bu, girdiler çok değişken uzunluklara sahip olduğunda çok fazla zaman ve işlem gücü tasarrufu sağlayabilir!\n\nTokenlaştırma işlevini tüm veri kümelerimize aynı anda nasıl uygulayacağımız aşağıda açıklanmıştır. Map çağrımızda batched=True kullanıyoruz, böylece işlev her bir öğeye ayrı ayrı değil, veri kümemizin birden çok öğesine aynı anda uygulanıyor. Bu, daha hızlı ön işleme yapılmasını sağlar.","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:41.228470Z","iopub.execute_input":"2024-08-07T06:23:41.228910Z","iopub.status.idle":"2024-08-07T06:23:42.274423Z","shell.execute_reply.started":"2024-08-07T06:23:41.228872Z","shell.execute_reply":"2024-08-07T06:23:42.273264Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b5efcd784949628a109aba1442ce0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63c6622dc5d42c486942304606b94bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b299a13aad4465791a37bfa08bef92f"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Datasets kütüphanesinin bu işlemi uygulama şekli, ön işleme fonksiyonu tarafından döndürülen sözlükteki her anahtar için bir tane olmak üzere veri kümelerine yeni alanlar eklemektir:","metadata":{}},{"cell_type":"markdown","source":"Hatta ön işleme fonksiyonunuzu map() ile uygularken bir num_proc argümanı ileterek çoklu işlemeyi kullanabilirsiniz. Burada bunu yapmadık çünkü Tokenizers kütüphanesi zaten örneklerimizi daha hızlı tokenize etmek için birden fazla iş parçacığı kullanıyor, ancak bu kütüphane tarafından desteklenen hızlı bir tokenizer kullanmıyorsanız, bu ön işlemenizi hızlandırabilir.\n\ntokenize_fonksiyonumuz input_ids, attention_mask ve token_type_ids anahtarlarını içeren bir sözlük döndürür, böylece bu üç alan veri kümemizin tüm bölünmelerine eklenir. Ön işleme fonksiyonumuz map() uyguladığımız veri kümesindeki mevcut bir anahtar için yeni bir değer döndürürse mevcut alanları da değiştirebileceğimizi unutmayın.\n\nYapmamız gereken son şey, öğeleri bir araya getirdiğimizde tüm örnekleri en uzun öğenin uzunluğuna kadar doldurmaktır - dinamik padding olarak adlandırdığımız bir teknik.","metadata":{}},{"cell_type":"markdown","source":"## Dynamic padding\n\nÖrnekleri bir yığın içinde bir araya getirmekten sorumlu olan işleve collate işlevi denir. Bu, bir DataLoader oluştururken geçebileceğiniz bir argümandır, varsayılan olarak örneklerinizi PyTorch tensörlerine dönüştüren ve bunları birleştiren bir fonksiyondur (öğeleriniz listeler, tuples veya sözlükler ise özyinelemeli olarak). Elimizdeki girdilerin hepsi aynı boyutta olmayacağı için bu bizim durumumuzda mümkün olmayacaktır. Dolguyu kasıtlı olarak erteledik, böylece her bir yığında yalnızca gerektiği kadar uygulayabilir ve çok fazla dolguya sahip aşırı uzun girdilere sahip olmaktan kaçınabiliriz. Bu, eğitimi oldukça hızlandıracaktır, ancak bir TPU üzerinde eğitim yapıyorsanız bunun sorunlara neden olabileceğini unutmayın - TPU'lar, ekstra dolgu gerektirse bile sabit şekilleri tercih eder.\n\nBunu pratikte yapmak için, bir araya getirmek istediğimiz veri kümesinin öğelerine doğru miktarda dolgu uygulayacak bir collate işlevi tanımlamamız gerekir. Neyse ki Transformers kütüphanesi DataCollatorWithPadding aracılığıyla bize böyle bir fonksiyon sağlıyor. Bu işlev, siz onu örneklediğinizde (hangi dolgu token'ının kullanılacağını ve modelin dolgunun girdilerin solunda mı yoksa sağında mı olmasını beklediğini bilmek için) bir tokenizer alır ve ihtiyacınız olan her şeyi yapar:","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:42.276021Z","iopub.execute_input":"2024-08-07T06:23:42.276393Z","iopub.status.idle":"2024-08-07T06:23:56.490164Z","shell.execute_reply.started":"2024-08-07T06:23:42.276362Z","shell.execute_reply":"2024-08-07T06:23:56.488993Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2024-08-07 06:23:44.934649: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-07 06:23:44.934787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-07 06:23:45.107636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Bu yeni oyuncağı test etmek için, eğitim setimizden bir araya getirmek istediğimiz birkaç örnek alalım. Burada, idx, sentence1 ve sentence2 sütunlarını gerekmeyeceği ve dizeler içerdiği için (ve dizelerle tensör oluşturamayacağımız için) kaldırıyoruz ve yığındaki her bir girdinin uzunluklarına bakıyoruz:","metadata":{}},{"cell_type":"code","source":"samples = tokenized_datasets[\"train\"][:8]\nsamples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n[len(x) for x in samples[\"input_ids\"]]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:56.494155Z","iopub.execute_input":"2024-08-07T06:23:56.494950Z","iopub.status.idle":"2024-08-07T06:23:56.506152Z","shell.execute_reply.started":"2024-08-07T06:23:56.494899Z","shell.execute_reply":"2024-08-07T06:23:56.504941Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[50, 59, 47, 67, 59, 50, 62, 32]"},"metadata":{}}]},{"cell_type":"markdown","source":"Sürpriz değil, 32'den 67'ye kadar değişen uzunlukta örnekler alıyoruz. Dinamik dolgu, bu gruptaki örneklerin hepsinin grup içindeki maksimum uzunluk olan 67 uzunluğa kadar doldurulması gerektiği anlamına gelir. Dinamik dolgu olmadan, tüm örneklerin tüm veri kümesindeki maksimum uzunluğa veya modelin kabul edebileceği maksimum uzunluğa doldurulması gerekirdi. data_collator'ımızın yığını dinamik olarak düzgün şekilde doldurduğunu iki kez kontrol edelim:","metadata":{}},{"cell_type":"code","source":"batch = data_collator(samples)\n{k: v.shape for k, v in batch.items()}","metadata":{"execution":{"iopub.status.busy":"2024-08-07T06:23:56.507410Z","iopub.execute_input":"2024-08-07T06:23:56.507752Z","iopub.status.idle":"2024-08-07T06:23:56.552406Z","shell.execute_reply.started":"2024-08-07T06:23:56.507723Z","shell.execute_reply":"2024-08-07T06:23:56.551018Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'input_ids': torch.Size([8, 67]),\n 'token_type_ids': torch.Size([8, 67]),\n 'attention_mask': torch.Size([8, 67]),\n 'labels': torch.Size([8])}"},"metadata":{}}]}]}