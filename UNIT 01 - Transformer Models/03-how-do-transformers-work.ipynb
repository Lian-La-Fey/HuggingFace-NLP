{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How do Transformers work?\n\nBu bölümde, Transformatör modellerinin mimarisine üst düzey bir bakış atacağız.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## A bit of Transformer history\n\nİşte Transformatör modellerinin (kısa) tarihindeki bazı referans noktaları:\n\n![image1](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg)\n\nTransformer mimarisi Haziran 2017'de tanıtıldı. Orijinal araştırmanın odak noktası çeviri görevleriydi. Bunu, aşağıdakiler de dahil olmak üzere birkaç etkili modelin tanıtılması izledi:\n\n- Haziran 2018: İlk ön eğitimli Transformer modeli olan GPT, çeşitli NLP görevlerinde ince ayar için kullanıldı ve son teknoloji ürünü sonuçlar elde etti\n\n- Ekim 2018: BERT, başka bir büyük ön eğitimli model, bu model cümlelerin daha iyi özetlerini üretmek için tasarlanmıştır (bu konuda daha fazlası bir sonraki bölümde!)\n\n- Şubat 2019: GPT'nin geliştirilmiş (ve daha büyük) bir versiyonu olan GPT-2, etik kaygılar nedeniyle hemen kamuya açıklanmadı\n\n- Ekim 2019: DistilBERT, BERT'in %60 daha hızlı, bellekte %40 daha hafif ve hala BERT'in performansının %97'sini koruyan damıtılmış bir sürümü\n\n- Ekim 2019: BART ve T5, orijinal Transformer modeliyle aynı mimariyi kullanan iki büyük ön eğitimli model (bunu yapan ilk model)\n\n- Mayıs 2020, GPT-2'nin daha da büyük bir versiyonu olan ve ince ayar gerektirmeden çeşitli görevlerde iyi performans gösterebilen GPT-3 (zero-shot learning olarak adlandırılır)\n\nBu liste kapsamlı olmaktan uzaktır ve sadece farklı Transformer modellerinden birkaçını vurgulamayı amaçlamaktadır. Genel olarak üç kategoride gruplandırılabilirler:\n\n- GPT benzeri (otomatik regresif Transformatör modelleri olarak da adlandırılır) \n- BERT benzeri (otomatik kodlama Transformatör modelleri olarak da adlandırılır) \n- BART/T5 benzeri (diziden diziye Transformatör modelleri olarak da adlandırılır) \n\nBu aileleri daha sonra daha derinlemesine inceleyeceğiz.","metadata":{}},{"cell_type":"markdown","source":"## Transformers are language models\n\nYukarıda bahsedilen tüm Transformatör modelleri (GPT, BERT, BART, T5, vb.) dil modelleri olarak eğitilmiştir. Bu, büyük miktarda ham metin üzerinde kendi kendine denetimli bir şekilde eğitildikleri anlamına gelir. Kendi kendine denetimli öğrenme, hedefin modelin girdilerinden otomatik olarak hesaplandığı bir eğitim türüdür. Bu, verileri etiketlemek için insanlara ihtiyaç olmadığı anlamına gelir!\n\n**Bu tür bir model, üzerinde eğitildiği dil hakkında istatistiksel bir anlayış geliştirir**, ancak belirli pratik görevler için çok kullanışlı değildir. Bu nedenle, genel ön eğitimli model daha sonra transfer öğrenme adı verilen bir süreçten geçer. Bu süreç sırasında model, belirli bir görev üzerinde denetimli bir şekilde - yani insan açıklamalı etiketler kullanılarak - ince ayarlanır.\n\nBir görev örneği, önceki n kelimeyi okuduktan sonra bir cümledeki bir sonraki kelimeyi tahmin etmektir. Buna nedensel dil modelleme denir çünkü çıktı geçmiş ve şimdiki girdilere bağlıdır, ancak gelecektekilere bağlı değildir.\n\n![image2](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg)\n\nBir başka örnek de, modelin cümle içinde maskelenmiş bir kelimeyi tahmin ettiği maskelenmiş dil modellemesidir.\n\n![image3](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg)","metadata":{}},{"cell_type":"markdown","source":"## Transformers are big models\n\nBirkaç aykırı değer (DistilBERT gibi) dışında, daha iyi performans elde etmek için genel strateji, modellerin boyutlarını ve ön eğitime tabi tutuldukları veri miktarını artırmaktır.\n\n![image4](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png)\n\nNe yazık ki bir modelin, özellikle de büyük bir modelin eğitilmesi büyük miktarda veri gerektirir. Bu da zaman ve hesaplama kaynakları açısından çok maliyetli hale gelmektedir. Hatta aşağıdaki grafikte görülebileceği gibi çevresel etkiye de dönüşür.\n\n![image5](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg)\n\nVe bu, bilinçli olarak ön eğitimin çevresel etkisini azaltmaya çalışan bir ekip tarafından yönetilen (çok büyük) bir model için bir projeyi göstermektedir. En iyi hiperparametreleri elde etmek için çok sayıda deneme yapmanın ayak izi daha da yüksek olacaktır.\n\nBir araştırma ekibinin, bir öğrenci organizasyonunun ya da bir şirketin bir modeli her eğitmek istediğinde bunu sıfırdan yaptığını düşünün. Bu çok büyük, gereksiz küresel maliyetlere yol açardı!\n\nİşte bu yüzden dil modellerinin paylaşılması çok önemlidir: eğitilmiş ağırlıkların paylaşılması ve daha önce eğitilmiş ağırlıkların üzerine inşa edilmesi, topluluğun genel hesaplama maliyetini ve karbon ayak izini azaltır.\n\nBu arada, modellerinizin eğitiminin karbon ayak izini çeşitli araçlar aracılığıyla değerlendirebilirsiniz. Örneğin ML CO2 Impact veya Transformers'a entegre edilmiş olan Code Carbon. Bu konuda daha fazla bilgi edinmek için, eğitiminizin ayak izini tahmin eden bir emisyon.csv dosyasını nasıl oluşturacağınızı gösteren bu blog gönderisini ve bu konuyu ele alan Transformers belgelerini okuyabilirsiniz.","metadata":{}},{"cell_type":"markdown","source":"## Transfer Learning\n\nÖn eğitim, bir modeli sıfırdan eğitme eylemidir: ağırlıklar rastgele başlatılır ve eğitim herhangi bir ön bilgi olmadan başlar.\n\n![image6](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg)\n\nBu ön eğitim genellikle çok büyük miktarda veri üzerinde yapılır. Bu nedenle, çok büyük bir veri külliyatı gerektirir ve eğitim birkaç haftaya kadar sürebilir.\n\nÖte yandan ince ayar, bir model ön eğitimden geçirildikten sonra yapılan eğitimdir. İnce ayar yapmak için önce önceden eğitilmiş bir dil modeli elde edersiniz, ardından görevinize özel bir veri kümesiyle ek eğitim gerçekleştirirsiniz. Bekle - neden nihai kullanım durumunuz için modeli en baştan (sıfırdan) eğitmiyorsunuz? Bunun birkaç nedeni vardır:\n\n- Ön eğitimli model, ince ayar veri kümesiyle bazı benzerlikleri olan bir veri kümesi üzerinde zaten eğitilmiştir. Böylece ince ayar süreci, ön eğitim sırasında ilk model tarafından edinilen bilgiden faydalanabilir (örneğin, NLP problemlerinde, ön eğitimli model, göreviniz için kullandığınız dil hakkında bir tür istatistiksel anlayışa sahip olacaktır).\n\n- Önceden eğitilmiş model zaten çok sayıda veri üzerinde eğitildiğinden, ince ayar iyi sonuçlar elde etmek için çok daha az veri gerektirir.\n\n- Aynı nedenle, iyi sonuçlar almak için gereken zaman ve kaynak miktarı çok daha düşüktür.\n\nÖrneğin, İngilizce dili üzerinde eğitilmiş ön eğitimli bir modelden yararlanılabilir ve daha sonra bir arXiv külliyatı üzerinde ince ayar yapılarak bilim/araştırma tabanlı bir model elde edilebilir. İnce ayar sadece sınırlı miktarda veri gerektirecektir: önceden eğitilmiş modelin edindiği bilgi \"aktarılır\", dolayısıyla transfer öğrenme terimi kullanılır.\n\n![image7](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg)\n\nBu nedenle bir modele ince ayar yapmanın zaman, veri, mali ve çevresel maliyetleri daha düşüktür. Ayrıca, eğitim tam bir ön eğitime göre daha az kısıtlayıcı olduğundan, farklı ince ayar şemaları üzerinde yineleme yapmak daha hızlı ve kolaydır.\n\nBu süreç aynı zamanda sıfırdan eğitimden daha iyi sonuçlar elde edecektir (çok fazla veriniz yoksa), bu nedenle her zaman önceden eğitilmiş bir modelden (elinizdeki göreve mümkün olduğunca yakın bir modelden) yararlanmaya çalışmalı ve ona ince ayar yapmalısınız.","metadata":{}},{"cell_type":"markdown","source":"## General architecture\n\nBu bölümde, Transformatör modelinin genel mimarisinin üzerinden geçeceğiz. Bazı kavramları anlamadıysanız endişelenmeyin; daha sonra bileşenlerin her birini kapsayan ayrıntılı bölümler var.","metadata":{}},{"cell_type":"markdown","source":"### Introduction\n\nModel temel olarak iki bloktan oluşmaktadır:\n\n- Kodlayıcı (solda): Kodlayıcı bir girdi alır ve bunun bir temsilini (özelliklerini) oluşturur. Bu, modelin girdiden anlama elde etmek için optimize edildiği anlamına gelir. \n\n- Kod çözücü (sağ): Kod çözücü, bir hedef dizisi oluşturmak için kodlayıcının temsilini (özelliklerini) diğer girdilerle birlikte kullanır. Bu, modelin çıktılar üretmek için optimize edildiği anlamına gelir.\n\n![image8](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg)\n\nBu parçaların her biri göreve bağlı olarak bağımsız olarak kullanılabilir:\n\n- Yalnızca kodlayıcı modeller: Cümle sınıflandırma ve adlandırılmış varlık tanıma gibi girdinin anlaşılmasını gerektiren görevler için iyidir. \n\n- Yalnızca kod çözücü modeller: Metin üretimi gibi üretken görevler için uygundur. \n\n- Kodlayıcı-kod çözücü modelleri veya diziden diziye modeller: Çeviri veya özetleme gibi girdi gerektiren üretken görevler için uygundur. Bu mimarileri ilerleyen bölümlerde bağımsız olarak inceleyeceğiz.","metadata":{}},{"cell_type":"markdown","source":"## Attention layers\n\nTransformer modellerinin önemli bir özelliği, dikkat katmanları adı verilen özel katmanlarla inşa edilmeleridir. Aslında Transformer mimarisini tanıtan makalenin başlığı  “Attention Is All You Need”! Dikkat katmanlarının ayrıntılarını kursun ilerleyen bölümlerinde inceleyeceğiz; **şimdilik bilmeniz gereken tek şey, bu katmanın modele, her bir kelimenin temsiliyle uğraşırken kendisine ilettiğiniz cümledeki belirli kelimelere özel dikkat göstermesini (ve diğerlerini aşağı yukarı görmezden gelmesini) söyleyeceğidir.**\n\nBunu bir bağlama oturtmak için metni İngilizce'den Fransızca'ya çevirme görevini düşünün. \"You like this course.\" girdisi verildiğinde, bir çeviri modelinin \"like\" kelimesinin doğru çevirisini elde etmek için bitişikteki \"You\" kelimesiyle de ilgilenmesi gerekecektir, çünkü Fransızcada \"like\" fiili özneye bağlı olarak farklı şekilde çekilir. Ancak cümlenin geri kalanı bu kelimenin çevirisi için kullanışlı değildir. Aynı şekilde, \"this\" kelimesini çevirirken modelin \"course\" kelimesine de dikkat etmesi gerekecektir, çünkü \"this\", ilgili ismin eril veya dişil olmasına bağlı olarak farklı şekilde çevrilir. Yine, cümledeki diğer kelimeler \"course\" kelimesinin çevirisi için önemli olmayacaktır. Daha karmaşık cümlelerde (ve daha karmaşık dilbilgisi kurallarında), modelin her bir kelimeyi doğru şekilde çevirmek için cümlede daha uzakta görünebilecek kelimelere özel dikkat göstermesi gerekecektir.\n\nAynı kavram doğal dille ilgili herhangi bir görev için de geçerlidir: bir kelimenin kendi başına bir anlamı vardır, ancak bu anlam, üzerinde çalışılan kelimeden önce veya sonra başka herhangi bir kelime (veya kelimeler) olabilen bağlamdan derinden etkilenir.\n\nArtık dikkat katmanlarının neyle ilgili olduğuna dair bir fikriniz olduğuna göre, Transformer mimarisine daha yakından bakalım.","metadata":{}},{"cell_type":"markdown","source":"## The original architecture\n\nTransformer mimarisi başlangıçta çeviri için tasarlanmıştır. Eğitim sırasında, kodlayıcı belirli bir dilde girdiler (cümleler) alırken, kod çözücü aynı cümleleri istenen hedef dilde alır. **Kodlayıcıda, dikkat katmanları bir cümledeki tüm kelimeleri kullanabilir** (çünkü az önce gördüğümüz gibi, belirli bir kelimenin çevirisi cümlede ondan önce olduğu kadar sonra olana da bağlı olabilir). **Ancak kod çözücü sıralı olarak çalışır ve yalnızca cümledeki daha önce çevirdiği kelimelere dikkat edebilir** (yani, yalnızca şu anda üretilmekte olan kelimeden önceki kelimeler). Örneğin, çevrilen hedefin ilk üç kelimesini tahmin ettiğimizde, bunları kod çözücüye veririz ve o da **dördüncü kelimeyi tahmin etmeye çalışmak için kodlayıcının tüm girdilerini kullanır**.\n\nEğitim sırasında işleri hızlandırmak için (modelin hedef cümlelere erişimi olduğunda), kod çözücü tüm hedefle beslenir, ancak gelecekteki kelimeleri kullanmasına izin verilmez (2. konumdaki kelimeyi tahmin etmeye çalışırken 2. konumdaki kelimeye erişimi olsaydı, sorun çok zor olmazdı!) Örneğin, dördüncü kelimeyi tahmin etmeye çalışırken, dikkat katmanı yalnızca 1 ila 3. konumlardaki kelimelere erişebilecektir.\n\nOrijinal Transformer mimarisi, kodlayıcı solda ve kod çözücü sağda olmak üzere şuna benziyordu:\n\n![image8](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg)\n\nBir kod çözücü bloğundaki ilk dikkat katmanının kod çözücüye giden tüm (geçmiş) girdilere dikkat ettiğini, ancak ikinci dikkat katmanının kodlayıcının çıktısını kullandığını unutmayın. Böylece mevcut kelimeyi en iyi şekilde tahmin etmek için tüm giriş cümlesine erişebilir. Bu çok kullanışlıdır çünkü farklı dillerde kelimeleri farklı sıralara koyan gramer kuralları olabilir veya cümlenin ilerleyen kısımlarında sağlanan bazı bağlamlar belirli bir kelimenin en iyi çevirisini belirlemek için yardımcı olabilir.\n\nDikkat maskesi, modelin bazı özel kelimelere dikkat etmesini önlemek için kodlayıcı/kod çözücüde de kullanılabilir - örneğin, cümleleri bir araya getirirken tüm girdileri aynı uzunlukta yapmak için kullanılan özel dolgu kelimesi.","metadata":{}},{"cell_type":"markdown","source":"## Architectures vs. checkpoints\n\nBu derste Transformer modellerini incelerken, modellerin yanı sıra mimariler ve kontrol noktalarından da bahsedildiğini göreceksiniz. Bu terimlerin hepsinin biraz farklı anlamları vardır:\n\n- Mimari: Bu, modelin iskeletidir - her katmanın ve model içinde gerçekleşen her işlemin tanımıdır. \n\n- Kontrol Noktaları: Bunlar belirli bir mimaride yüklenecek ağırlıklardır. \n\n- Model: Bu, \"mimari\" veya \"kontrol noktası\" kadar kesin olmayan bir şemsiye terimdir: her ikisi anlamına da gelebilir. Bu kurs, belirsizliği azaltmak için önemli olduğunda mimari veya kontrol noktasını belirtecektir. \n\nÖrneğin, BERT bir mimaridir ve BERT'in ilk sürümü için Google ekibi tarafından eğitilen bir ağırlık kümesi olan bert-base-cased bir kontrol noktasıdır. Bununla birlikte, \"BERT modeli\" ve \"bert-base-cased modeli\" denilebilir.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}