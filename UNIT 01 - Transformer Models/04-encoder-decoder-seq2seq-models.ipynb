{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Encoder models\n\nKodlayıcı modeller yalnızca bir Transformatör modelinin kodlayıcısını kullanır. Her aşamada, dikkat katmanları başlangıç cümlesindeki tüm kelimelere erişebilir. Bu modeller genellikle \"çift yönlü\" dikkate sahip olarak nitelendirilir ve genellikle otomatik kodlama modelleri olarak adlandırılır.\n\nBu modellerin ön eğitimi genellikle belirli bir cümlenin bir şekilde bozulması (örneğin, içindeki rastgele kelimeleri maskeleyerek) ve modele ilk cümleyi bulma veya yeniden oluşturma görevi verilmesi etrafında döner.\n\nKodlayıcı modeller, cümle sınıflandırma, adlandırılmış varlık tanıma (ve daha genel olarak kelime sınıflandırma) ve çıkarımsal soru yanıtlama gibi cümlenin tamamının anlaşılmasını gerektiren görevler için en uygun modellerdir.\n\nBu model ailesinin temsilcileri şunları içerir:\n\n- ALBERT\n- BERT \n- DistilBERT \n- ELECTRA \n- RoBERTa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Decoder Models\n\nKod çözücü modeller sadece bir Transformer modelinin kod çözücüsünü kullanır. Her aşamada, belirli bir kelime için dikkat katmanları yalnızca cümlede kendisinden önce konumlandırılmış kelimelere erişebilir. Bu modeller genellikle oto-regresif modeller olarak adlandırılır.\n\nKod çözücü modellerin ön eğitimi genellikle cümledeki bir sonraki kelimeyi tahmin etme etrafında döner.\n\nBu modeller metin oluşturmayı içeren görevler için en uygun modellerdir.\n\nBu model ailesinin temsilcileri şunları içerir:\n\n- CTRL \n- GPT \n- GPT-2 \n- Transformer XL","metadata":{}},{"cell_type":"markdown","source":"# Sequence-to-sequence models sequence-to-sequence-models\n\nKodlayıcı-kod çözücü modelleri (diziden diziye modeller olarak da adlandırılır) Transformatör mimarisinin her iki bölümünü de kullanır. Her aşamada, kodlayıcının dikkat katmanları başlangıç cümlesindeki tüm kelimelere erişebilirken, kod çözücünün dikkat katmanları yalnızca girdideki belirli bir kelimeden önce konumlandırılan kelimelere erişebilir.\n\nBu modellerin ön eğitimi, kodlayıcı veya kod çözücü modellerin hedefleri kullanılarak yapılabilir, ancak genellikle biraz daha karmaşık bir şey içerir. Örneğin, T5 rastgele metin aralıklarını (birkaç kelime içerebilir) tek bir maske özel kelime ile değiştirerek ön eğitime tabi tutulur ve amaç daha sonra bu maske kelimenin değiştirdiği metni tahmin etmektir.\n\nDiziden diziye modeller, özetleme, çeviri veya üretken soru yanıtlama gibi belirli bir girdiye bağlı olarak yeni cümleler üretme etrafında dönen görevler için en uygun olanlardır.\n\nBu model ailesinin temsilcileri şunları içerir:\n\n- BART \n- mBART \n- Marian \n- T5","metadata":{}}]}