{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenizers\n\nTokenizer'lar NLP pipeline'nın temel bileşenlerinden biridir. Tek bir amaca hizmet ederler: **metni model tarafından işlenebilecek verilere çevirmek**. Modeller yalnızca sayıları işleyebilir, bu nedenle tokenizer'ların metin girdilerimizi sayısal verilere dönüştürmesi gerekir. Bu bölümde, tokenizasyon işlem hattında tam olarak ne olduğunu inceleyeceğiz.\n\nNLP görevlerinde, genellikle işlenen veriler ham metindir. İşte böyle bir metin örneği:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"> Jim Henson was a puppeteer","metadata":{}},{"cell_type":"markdown","source":"Ancak, modeller yalnızca sayıları işleyebilir, bu nedenle ham metni sayılara dönüştürmenin bir yolunu bulmamız gerekir. Tokenizer'ların yaptığı da budur ve bunu yapmanın pek çok yolu vardır. Amaç, en anlamlı temsili - yani model için en anlamlı olanı - ve mümkünse en küçük temsili bulmaktır.\n\nŞimdi bazı tokenizasyon algoritması örneklerine bir göz atalım ve tokenizasyon hakkında aklınıza gelebilecek bazı soruları yanıtlamaya çalışalım.","metadata":{}},{"cell_type":"markdown","source":"## Word-based\n\nAkla gelen ilk tokenizer türü kelime tabanlıdır. Genellikle yalnızca birkaç kuralla kurulumu ve kullanımı çok kolaydır ve genellikle iyi sonuçlar verir. Örneğin, aşağıdaki resimde amaç ham metni kelimelere ayırmak ve her biri için sayısal bir temsil bulmaktır:\n\n![image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n\nMetni bölmenin farklı yolları vardır. Örneğin, Python'un **split()** fonksiyonunu uygulayarak metni kelimelere ayırmak için boşlukları kullanabiliriz:","metadata":{}},{"cell_type":"code","source":"tokenized_text = \"Jim Henson was a puppeteer\".split()\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:19:02.075977Z","iopub.execute_input":"2024-08-02T06:19:02.076501Z","iopub.status.idle":"2024-08-02T06:19:02.120426Z","shell.execute_reply.started":"2024-08-02T06:19:02.076455Z","shell.execute_reply":"2024-08-02T06:19:02.119340Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['Jim', 'Henson', 'was', 'a', 'puppeteer']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Noktalama işaretleri için ekstra kurallara sahip kelime tokenizer çeşitleri de vardır. Bu tür bir tokenizer ile oldukça büyük \"kelime hazineleri\" elde edebiliriz; burada bir kelime hazinesi, külliyatımızda sahip olduğumuz toplam bağımsız token sayısı ile tanımlanır.\n\nHer kelimeye 0'dan başlayıp kelime dağarcığının boyutuna kadar giden bir kimlik atanır. Model, her kelimeyi tanımlamak için bu kimlikleri kullanır.\n\nKelime tabanlı bir belirteçleyici ile bir dili tamamen kapsamak istiyorsak, dildeki her kelime için bir tanımlayıcıya sahip olmamız gerekir ki bu da çok büyük miktarda token üretecektir. Örneğin, İngilizce dilinde 500.000'den fazla kelime vardır, bu nedenle her kelimeden bir girdi kimliğine bir harita oluşturmak için bu kadar çok kimliği takip etmemiz gerekir. Ayrıca, \"dog\" gibi kelimeler \"dogs\" gibi kelimelerden farklı şekilde temsil edilir ve modelin başlangıçta \"dog\" ve \"dogs\" kelimelerinin benzer olduğunu bilmesinin hiçbir yolu yoktur: iki kelimeyi ilgisiz olarak tanımlayacaktır. Aynı durum, modelin başlangıçta benzer olarak görmeyeceği \"run\" ve \"running\" gibi diğer benzer kelimeler için de geçerlidir.\n\nSon olarak, kelime dağarcığımızda olmayan kelimeleri temsil etmek için özel bir belirtece ihtiyacımız var. Bu, \"bilinmeyen\" token olarak bilinir ve genellikle \"[UNK]\" veya \"\\<unk\\>\" olarak gösterilir. Tokenizer'ın bu tokenlardan çok sayıda ürettiğini görürseniz bu genellikle kötüye işarettir, çünkü bir kelimenin mantıklı bir temsilini alamamıştır ve yol boyunca bilgi kaybediyorsunuzdur. Sözcük dağarcığını oluştururken amaç, tokenizer'ın mümkün olduğunca az sayıda sözcüğü bilinmeyen tokena dönüştürmesini sağlamaktır.\n\nBilinmeyen token miktarını azaltmanın bir yolu, karakter tabanlı bir tokenizer kullanarak bir seviye daha derine inmektir.","metadata":{}},{"cell_type":"markdown","source":"## Character-based\n\nKarakter tabanlı tokenizer'lar metni kelimeler yerine karakterlere böler. Bunun iki temel faydası vardır:\n\n- Sözcük dağarcığı çok daha küçüktür.\n- Her kelime karakterlerden oluşturulabildiği için çok daha az kelime dağarcığı dışı (bilinmeyen) token vardır.\n\nAncak burada da boşluklar ve noktalama işaretleriyle ilgili bazı sorular ortaya çıkmaktadır:\n\n![eimag](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)\n\nBu yaklaşım da mükemmel değildir. Temsil artık kelimeler yerine karakterlere dayandığından, sezgisel olarak daha az anlamlı olduğu iddia edilebilir: her bir karakter tek başına çok fazla anlam ifade etmez, oysa kelimeler için durum böyledir. Ancak bu durum yine dile göre farklılık göstermektedir; örneğin Çince'de her bir karakter Latin dilindeki bir karakterden daha fazla bilgi taşımaktadır.\n\nDikkate alınması gereken bir diğer husus da modelimiz tarafından işlenecek çok büyük miktarda token elde edeceğimizdir: kelime tabanlı bir tokenizer ile bir kelime yalnızca tek bir token olurken, karakterlere dönüştürüldüğünde kolayca 10 veya daha fazla tokena dönüşebilir.\n\nHer iki dünyanın da en iyisini elde etmek için, iki yaklaşımı birleştiren üçüncü bir teknik kullanabiliriz: **subword tokenization**.","metadata":{}},{"cell_type":"markdown","source":"## Subword tokenization\n\nSubword tokenization algoritmaları, **sık kullanılan kelimelerin daha küçük alt kelimelere bölünmemesi gerektiği**, **ancak nadir kelimelerin anlamlı alt kelimelere ayrıştırılması gerektiği ilkesine dayanır**.\n\nÖrneğin, \"annoyingly\" nadir bir kelime olarak kabul edilebilir ve \"annoying\" ve \"ly\" olarak ayrıştırılabilir. Bunların her ikisinin de bağımsız alt kelimeler olarak daha sık görünmesi muhtemeldir, ancak aynı zamanda \"annoyingly\" kelimesinin anlamı \"annoying\" ve \"ly\" kelimelerinin bileşik anlamı tarafından korunur.\n\nAşağıda, bir alt kelime tokenizasyon algoritmasının \"Let's do tokenization!\" dizisini nasıl tokenize edeceğini gösteren bir örnek verilmiştir:\n\n![image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg)\n\nBu alt sözcükler çok fazla semantik anlam sağlamaktadır: örneğin, yukarıdaki örnekte \"tokenization\" sözcüğü \"token\" ve \"ization\" sözcüklerine bölünmüştür; bu iki sözcük hem semantik anlam taşımakta hem de alan açısından verimli olmaktadır (uzun bir sözcüğü temsil etmek için yalnızca iki token gereklidir). Bu, küçük kelime dağarcıklarıyla nispeten iyi bir kapsama alanına sahip olmamızı ve neredeyse hiç bilinmeyen token olmamasını sağlar.\n\nBu yaklaşım özellikle, alt kelimeleri bir araya getirerek (neredeyse) keyfi olarak uzun karmaşık kelimeler oluşturabileceğiniz Türkçe gibi sondan eklemeli dillerde kullanışlıdır.\n\n### Ve daha fazlası!\n\nŞaşırtıcı olmayan bir şekilde, daha birçok teknik var. Birkaçını saymak gerekirse:\n\n- GPT-2'de kullanılan bayt düzeyinde BPE\n- WordPiece, BERT'te kullanıldığı gibi\n- Birkaç çok dilli modelde kullanıldığı gibi SentencePiece veya Unigram\n\nArtık API'yi kullanmaya başlamak için tokenizer'ların nasıl çalıştığı hakkında yeterli bilgiye sahip olmalısınız.","metadata":{}},{"cell_type":"markdown","source":"## Loading and saving\n\nTokenizer'ları yüklemek ve kaydetmek modellerde olduğu kadar basittir. Aslında, aynı iki metoda dayanır: from_pretrained() ve save_pretrained(). Bu metotlar, tokenizer tarafından kullanılan algoritmayı (modelin mimarisi gibi) ve kelime dağarcığını (modelin ağırlıkları gibi) yükler veya kaydeder.\n\nBERT ile aynı kontrol noktası ile eğitilen BERT tokenizer'ın yüklenmesi, BertTokenizer sınıfını kullanmamız dışında, modelin yüklenmesi ile aynı şekilde yapılır:","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:32:29.969522Z","iopub.execute_input":"2024-08-02T06:32:29.970016Z","iopub.status.idle":"2024-08-02T06:32:35.949529Z","shell.execute_reply.started":"2024-08-02T06:32:29.969977Z","shell.execute_reply":"2024-08-02T06:32:35.948359Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf4669d6f064713890edb25213760f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d5ee56dce24ef3b03a537fe857540c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939fdb38e23e4b2987785936a054a82a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967cf3aaa75b4851be2674a6117abe9a"}},"metadata":{}}]},{"cell_type":"markdown","source":"AutoModel'e benzer şekilde, AutoTokenizer sınıfı, kontrol noktası adına göre kütüphanedeki uygun tokenizer sınıfını alır ve herhangi bir kontrol noktasıyla doğrudan kullanılabilir:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:33:47.925701Z","iopub.execute_input":"2024-08-02T06:33:47.926239Z","iopub.status.idle":"2024-08-02T06:33:48.105243Z","shell.execute_reply.started":"2024-08-02T06:33:47.926206Z","shell.execute_reply":"2024-08-02T06:33:48.103814Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Artık tokenizer'ı önceki bölümde gösterildiği gibi kullanabiliriz:","metadata":{}},{"cell_type":"code","source":"tokenizer(\"Using a Transformer network is simple\")","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:34:11.560569Z","iopub.execute_input":"2024-08-02T06:34:11.560959Z","iopub.status.idle":"2024-08-02T06:34:11.572068Z","shell.execute_reply.started":"2024-08-02T06:34:11.560928Z","shell.execute_reply":"2024-08-02T06:34:11.570732Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:35:17.114700Z","iopub.execute_input":"2024-08-02T06:35:17.115099Z","iopub.status.idle":"2024-08-02T06:35:17.143115Z","shell.execute_reply.started":"2024-08-02T06:35:17.115066Z","shell.execute_reply":"2024-08-02T06:35:17.141956Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/vocab.txt',\n 'tokenizer/added_tokens.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"Bölüm 3'te token_type_ids hakkında daha fazla konuşacağız ve attention_mask anahtarını biraz sonra açıklayacağız. İlk olarak, input_id'lerin nasıl oluşturulduğunu görelim. Bunu yapmak için, tokenizer'ın ara yöntemlerine bakmamız gerekecek.","metadata":{}},{"cell_type":"markdown","source":"## Encoding\n\nMetni sayılara çevirmek kodlama olarak bilinir. Kodlama iki aşamalı bir işlemle yapılır: tokenizasyon ve ardından giriş kimliklerine dönüştürme.\n\nGördüğümüz gibi, ilk adım metni genellikle token olarak adlandırılan kelimelere (veya kelimelerin bölümlerine, noktalama işaretlerine vb.) ayırmaktır. Bu süreci yönetebilecek birden fazla kural vardır, bu nedenle model ön eğitimden geçirilirken kullanılan kuralların aynısını kullandığımızdan emin olmak için modelin adını kullanarak tokenizer'ı örneklememiz gerekir.\n\nİkinci adım, bu tokenları sayılara dönüştürmektir, böylece onlardan bir tensör oluşturabilir ve modele besleyebiliriz. Bunu yapmak için, tokenizer'ın bir kelime dağarcığı vardır; bu, onu from_pretrained() yöntemiyle örneklediğimizde indirdiğimiz kısımdır. Yine, model ön eğitimden geçirilirken kullanılan kelime dağarcığının aynısını kullanmamız gerekir.\n\nİki adımı daha iyi anlamak için bunları ayrı ayrı inceleyeceğiz. Size bu adımların ara sonuçlarını göstermek için tokenizasyon pipeline'ının parçalarını ayrı ayrı gerçekleştiren bazı yöntemler kullanacağımızı unutmayın, ancak pratikte tokenizer'ı doğrudan girdileriniz üzerinde çağırmalısınız (Bölüm 2'de gösterildiği gibi).","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nsequence = \"Using a Transformer network is simple\"\ntokens = tokenizer.tokenize(sequence)\n\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:39:04.254249Z","iopub.execute_input":"2024-08-02T06:39:04.254616Z","iopub.status.idle":"2024-08-02T06:39:04.466975Z","shell.execute_reply.started":"2024-08-02T06:39:04.254590Z","shell.execute_reply":"2024-08-02T06:39:04.465520Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Bu tokenizer bir alt kelime tokenizeridir: kelime dağarcığı tarafından temsil edilebilecek tokenlar elde edene kadar kelimeleri böler. Burada transformer kelimesi iki tokena ayrılmıştır: transform ve ##er.","metadata":{}},{"cell_type":"markdown","source":"### From tokens to input IDs\n\nGiriş kimliklerine dönüştürme convert_tokens_to_ids() tokenizer yöntemi tarafından gerçekleştirilir:","metadata":{}},{"cell_type":"code","source":"ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:40:28.704541Z","iopub.execute_input":"2024-08-02T06:40:28.704962Z","iopub.status.idle":"2024-08-02T06:40:28.711064Z","shell.execute_reply.started":"2024-08-02T06:40:28.704929Z","shell.execute_reply":"2024-08-02T06:40:28.709707Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[7993, 170, 13809, 23763, 2443, 1110, 3014]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Bu çıktılar, uygun framework tensörüne dönüştürüldükten sonra, bu bölümde daha önce görüldüğü gibi bir modelin girdileri olarak kullanılabilir.","metadata":{}},{"cell_type":"markdown","source":"## Decoding\n\nKod çözme işlemi tam tersi şekilde gerçekleşir: kelime indislerinden bir dize elde etmek isteriz. Bu decode() metodu ile aşağıdaki gibi yapılabilir:","metadata":{}},{"cell_type":"code","source":"decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\nprint(decoded_string)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T06:41:56.665404Z","iopub.execute_input":"2024-08-02T06:41:56.665778Z","iopub.status.idle":"2024-08-02T06:42:11.489260Z","shell.execute_reply.started":"2024-08-02T06:41:56.665751Z","shell.execute_reply":"2024-08-02T06:42:11.487929Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-08-02 06:41:59.051298: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-02 06:41:59.051435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-02 06:41:59.199975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using a transformer network is simple\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**decode** yönteminin yalnızca dizinleri tekrar belirteçlere dönüştürmekle kalmadığını, aynı zamanda okunabilir bir cümle üretmek için aynı kelimelerin parçası olan belirteçleri bir araya getirdiğini unutmayın. Bu davranış, yeni metin tahmin eden modeller kullandığımızda (bir istemden oluşturulan metin ya da çeviri veya özetleme gibi diziden diziye problemler için) son derece faydalı olacaktır.\n\nŞimdiye kadar bir tokenizer'ın işleyebileceği atomik işlemleri anlamış olmalısınız: tokenizasyon, ID'lere dönüştürme ve ID'leri bir dizeye geri dönüştürme. Ancak, buzdağının sadece görünen kısmını kazıyabildik. Bir sonraki bölümde, yaklaşımımızı sınırlarına götüreceğiz ve bunların üstesinden nasıl geleceğimize bir göz atacağız.","metadata":{}}]}