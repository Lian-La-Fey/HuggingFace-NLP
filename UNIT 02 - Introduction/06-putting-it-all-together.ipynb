{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Putting it all together\n\nSon birkaç bölümde, işin çoğunu elle yapmak için elimizden gelenin en iyisini yapmaya çalıştık. Tokenizer'ların nasıl çalıştığını keşfettik ve tokenizasyon, girdi kimliklerine dönüştürme, padding, truncation ve dikkat maskelerine baktık.\n\nAncak, bölüm 2'de gördüğümüz gibi, Transformers API tüm bunları burada inceleyeceğimiz üst düzey bir işlevle bizim için halledebilir. Tokenizer'ınızı doğrudan cümle üzerinde çağırdığınızda, modelinizden geçmeye hazır girdileri geri alırsınız:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)\nmodel_inputs ","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:11:38.437403Z","iopub.execute_input":"2024-08-02T07:11:38.437772Z","iopub.status.idle":"2024-08-02T07:11:44.086741Z","shell.execute_reply.started":"2024-08-02T07:11:38.437743Z","shell.execute_reply":"2024-08-02T07:11:44.085430Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31abc3e04c354833ae7bd76145466ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e1c2b4227824942b869e2590ff36aec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0cf6014d6794193badf15ea01c21bf3"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"markdown","source":"Burada, model_inputs değişkeni bir modelin iyi çalışması için gerekli olan her şeyi içerir. DistilBERT için bu, girdi kimliklerinin yanı sıra dikkat maskesini de içerir. Ek girdileri kabul eden diğer modellerde de tokenizer nesnesi tarafından bu girdilerin çıktısı alınacaktır.\n\nAşağıda bazı örneklerde göreceğimiz gibi, bu yöntem çok güçlüdür. İlk olarak, tek bir diziyi tokenize edebilir:","metadata":{}},{"cell_type":"code","source":"sequences = [\n    \"I've been waiting for a HuggingFace course my whole life.\", \n    \"So have I!\"\n]\nmodel_inputs = tokenizer(sequences)\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:14:21.277676Z","iopub.execute_input":"2024-08-02T07:14:21.278083Z","iopub.status.idle":"2024-08-02T07:14:21.286656Z","shell.execute_reply.started":"2024-08-02T07:14:21.278050Z","shell.execute_reply":"2024-08-02T07:14:21.285311Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"},"metadata":{}}]},{"cell_type":"markdown","source":"Çeşitli hedeflere göre padding uygulayabilir:","metadata":{}},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, padding=\"longest\")\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:14:54.573377Z","iopub.execute_input":"2024-08-02T07:14:54.573791Z","iopub.status.idle":"2024-08-02T07:14:54.582558Z","shell.execute_reply.started":"2024-08-02T07:14:54.573760Z","shell.execute_reply":"2024-08-02T07:14:54.581444Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, padding=\"max_length\")\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:15:22.444201Z","iopub.execute_input":"2024-08-02T07:15:22.445685Z","iopub.status.idle":"2024-08-02T07:15:22.453431Z","shell.execute_reply.started":"2024-08-02T07:15:22.445645Z","shell.execute_reply":"2024-08-02T07:15:22.452306Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:15:39.461542Z","iopub.execute_input":"2024-08-02T07:15:39.462461Z","iopub.status.idle":"2024-08-02T07:15:39.470300Z","shell.execute_reply.started":"2024-08-02T07:15:39.462419Z","shell.execute_reply":"2024-08-02T07:15:39.469079Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, truncation=True)\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:16:16.041206Z","iopub.execute_input":"2024-08-02T07:16:16.042217Z","iopub.status.idle":"2024-08-02T07:16:16.049436Z","shell.execute_reply.started":"2024-08-02T07:16:16.042170Z","shell.execute_reply":"2024-08-02T07:16:16.048289Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, max_length=8, truncation=True)\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:16:24.630245Z","iopub.execute_input":"2024-08-02T07:16:24.630728Z","iopub.status.idle":"2024-08-02T07:16:24.638035Z","shell.execute_reply.started":"2024-08-02T07:16:24.630695Z","shell.execute_reply":"2024-08-02T07:16:24.636928Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:17:35.778988Z","iopub.execute_input":"2024-08-02T07:17:35.779447Z","iopub.status.idle":"2024-08-02T07:17:35.829003Z","shell.execute_reply.started":"2024-08-02T07:17:35.779413Z","shell.execute_reply":"2024-08-02T07:17:35.827783Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:17:57.794730Z","iopub.execute_input":"2024-08-02T07:17:57.795145Z","iopub.status.idle":"2024-08-02T07:18:12.628097Z","shell.execute_reply.started":"2024-08-02T07:17:57.795113Z","shell.execute_reply":"2024-08-02T07:18:12.626719Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"2024-08-02 07:18:00.037000: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-02 07:18:00.037156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-02 07:18:00.223171: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\narray([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n        12172,  2607,  2026,  2878,  2166,  1012,   102],\n       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"},"metadata":{}}]},{"cell_type":"code","source":"model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:18:12.629947Z","iopub.execute_input":"2024-08-02T07:18:12.630609Z","iopub.status.idle":"2024-08-02T07:18:12.639750Z","shell.execute_reply.started":"2024-08-02T07:18:12.630574Z","shell.execute_reply":"2024-08-02T07:18:12.638469Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n        12172,  2607,  2026,  2878,  2166,  1012,   102],\n       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Special tokens\n\nTokenizer tarafından döndürülen girdi kimliklerine bakarsak, daha önce sahip olduğumuzdan biraz farklı olduklarını göreceğiz:","metadata":{}},{"cell_type":"code","source":"sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)\nprint(model_inputs[\"input_ids\"])\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:18:48.003295Z","iopub.execute_input":"2024-08-02T07:18:48.004156Z","iopub.status.idle":"2024-08-02T07:18:48.012564Z","shell.execute_reply.started":"2024-08-02T07:18:48.004121Z","shell.execute_reply":"2024-08-02T07:18:48.011266Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Bir token kimliği başlangıçta ve bir diğeri de sona eklenmiştir. Bunun neyle ilgili olduğunu görmek için yukarıdaki iki kimlik dizisinin kodunu çözelim:","metadata":{}},{"cell_type":"code","source":"print(tokenizer.decode(model_inputs[\"input_ids\"]))\nprint(tokenizer.decode(ids))","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:19:56.313088Z","iopub.execute_input":"2024-08-02T07:19:56.313555Z","iopub.status.idle":"2024-08-02T07:19:56.320328Z","shell.execute_reply.started":"2024-08-02T07:19:56.313518Z","shell.execute_reply":"2024-08-02T07:19:56.318898Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\ni've been waiting for a huggingface course my whole life.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Tokenizer, [CLS] özel kelimesini başa ve [SEP] özel kelimesini sona eklemiştir. Bunun nedeni modelin bunlarla önceden eğitilmiş olmasıdır, bu nedenle çıkarımda aynı sonuçları elde etmek için bunları da eklememiz gerekir. Bazı modellerin özel kelimeler eklemediğini veya farklı kelimeler eklediğini unutmayın; modeller bu özel kelimeleri yalnızca başa veya yalnızca sona da ekleyebilir. Her durumda, tokenizer hangilerinin beklendiğini bilir ve bunu sizin için halleder.","metadata":{}},{"cell_type":"markdown","source":"## Wrapping up: From tokenizer to model\n\nŞimdi tokenizer nesnesinin metinlere uygulandığında kullandığı tüm bireysel adımları gördüğümüze göre, ana API'si ile çoklu dizileri (padding!), çok uzun dizileri (truncation!) ve çoklu tensör türlerini nasıl işleyebileceğini son bir kez görelim:","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T07:22:28.553001Z","iopub.execute_input":"2024-08-02T07:22:28.553442Z","iopub.status.idle":"2024-08-02T07:22:31.658504Z","shell.execute_reply.started":"2024-08-02T07:22:28.553411Z","shell.execute_reply":"2024-08-02T07:22:31.657293Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f1691684e0417da4a527684432bdfe"}},"metadata":{}}]}]}