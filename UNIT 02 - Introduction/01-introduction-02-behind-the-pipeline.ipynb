{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nB繹l羹m 1'de g繹rd羹羹n羹z gibi, Transformat繹r modelleri genellikle 癟ok b羹y羹kt羹r. Milyonlarca ila on milyarlarca parametre i癟eren bu modelleri eitmek ve da覺tmak karma覺k bir itir. Ayr覺ca, neredeyse her g羹n yeni modellerin piyasaya s羹r羹lmesi ve her birinin kendi uygulamas覺na sahip olmas覺 nedeniyle, hepsini denemek kolay bir i deildir.\n\nTransformers k羹t羹phanesi bu sorunu 癟繹zmek i癟in oluturulmutur. Amac覺, herhangi bir Transformer modelinin y羹klenebilecei, eitilebilecei ve kaydedilebilecei tek bir API salamakt覺r. K羹t羹phanenin ana 繹zellikleri unlard覺r:\n\n- Kullan覺m kolayl覺覺: Son teknoloji 羹r羹n羹 bir NLP modelinin indirilmesi, y羹klenmesi ve 癟覺kar覺m i癟in kullan覺lmas覺 sadece iki sat覺r kodla yap覺labilir. \n\n- Esneklik: z羹nde, t羹m modeller basit PyTorch nn.Module veya TensorFlow tf.keras.Model s覺n覺flar覺d覺r ve ilgili makine 繹renimi (ML) 癟er癟evelerindeki dier modeller gibi ele al覺nabilir. \n\n- Basitlik: K羹t羹phane genelinde neredeyse hi癟 soyutlama yap覺lmam覺t覺r. \"Hepsi tek bir dosyada\" temel bir kavramd覺r: bir modelin ileri ge癟ii tamamen tek bir dosyada tan覺mlan覺r, b繹ylece kodun kendisi anla覺labilir ve hacklenebilir. \n\nBu son 繹zellik Transformers'覺 dier ML k羹t羹phanelerinden olduk癟a farkl覺 k覺lar. Modeller, dosyalar aras覺nda payla覺lan mod羹ller 羹zerine ina edilmemitir; bunun yerine, her modelin kendi katmanlar覺 vard覺r. Bu, modelleri daha ula覺labilir ve anla覺labilir k覺lman覺n yan覺 s覺ra, bir model 羹zerinde dierlerini etkilemeden kolayca deney yapabilmenizi salar.\n\nBu b繹l羹m, B繹l羹m 1'de tan覺t覺lan pipeline() fonksiyonunu kopyalamak i癟in bir model ve bir tokenizer'覺 birlikte kulland覺覺m覺z u癟tan uca bir 繹rnekle ba\"\"layacakt覺r. Ard覺ndan, model API'sini tart覺aca覺z: model ve yap覺land覺rma s覺n覺flar覺na dalaca覺z ve bir modelin nas覺l y羹kleneceini ve tahminlerin 癟覺kt覺s覺n覺 almak i癟in say覺sal girdileri nas覺l ilediini g繹stereceiz.\n\nArd覺ndan pipeline() fonksiyonunun dier ana bileeni olan tokenizer API'sine bakaca覺z. Tokenizer'lar, sinir a覺 i癟in metinden say覺sal girdilere d繹n羹t羹rme ve gerektiinde metne geri d繹n羹t羹rme ilemlerini ger癟ekletirerek ilk ve son ileme ad覺mlar覺yla ilgilenir. Son olarak, birden fazla c羹mlenin bir model arac覺l覺覺yla haz覺rlanm覺 bir y覺覺n halinde nas覺l g繹nderileceini g繹stereceiz ve ard覺ndan 羹st d羹zey tokenizer() ilevine daha yak覺ndan bakarak her eyi tamamlayaca覺z.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Behind the pipeline\n\nB繹l羹m 1'de aa覺daki kodu 癟al覺t覺rd覺覺m覺zda perde arkas覺nda neler olduuna bir g繹z atarak tam bir 繹rnekle balayal覺m:","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\n    [\n        \"I've been waiting for a HuggingFace course my whole life.\",\n        \"I hate this so much!\",\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T19:11:32.014763Z","iopub.execute_input":"2024-08-01T19:11:32.015240Z","iopub.status.idle":"2024-08-01T19:12:05.078432Z","shell.execute_reply.started":"2024-08-01T19:11:32.015206Z","shell.execute_reply":"2024-08-01T19:12:05.076241Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-01 19:11:42.230271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-01 19:11:42.230460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-01 19:11:42.441068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88248b8d10184a36a8cb955fe7541739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5925c64d4b7c4f65b9b8667127415639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5c36663c4041d296a9ee1403aa8e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a180104541544c799d3e2113c6d557c"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"},"metadata":{}}]},{"cell_type":"markdown","source":"B繹l羹m 1'de g繹rd羹羹m羹z gibi, bu pipeline 羹癟 ad覺m覺 bir araya getirir: 繹n ileme, girdileri modelden ge癟irme ve son ileme:\n\n![image1](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg)","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing with a tokenizer\n\nDier sinir alar覺 gibi Transformer modelleri de ham metni dorudan ileyemez, bu nedenle boru hatt覺m覺z覺n ilk ad覺m覺 metin girdilerini modelin anlamland覺rabilecei say覺lara d繹n羹t羹rmektir. Bunu yapmak i癟in aa覺dakilerden sorumlu olacak bir tokenizer kullan覺yoruz:\n\n- Girdiyi token ad覺 verilen kelimelere, alt kelimelere veya sembollere (noktalama iaretleri gibi) b繹lme\n- Her bir tokeni bir tamsay覺ya eleme\n- Modele faydal覺 olabilecek ek girdilerin eklenmesi\n\n\nT羹m bu 繹n ilemlerin modelin 繹n eitime tabi tutulmas覺yla tamamen ayn覺 ekilde yap覺lmas覺 gerekir, bu nedenle 繹ncelikle bu bilgileri Model Hub'覺ndan indirmemiz gerekir. Bunu yapmak i癟in AutoTokenizer s覺n覺f覺n覺 ve onun from_pretrained() y繹ntemini kullan覺r覺z. Modelimizin kontrol noktas覺 ad覺n覺 kullanarak, modelin tokenizer'覺yla ilikili verileri otomatik olarak getirecek ve 繹nbellee alacakt覺r (b繹ylece yaln覺zca aa覺daki kodu ilk kez 癟al覺t覺rd覺覺n覺zda indirilir).\n\nSentiment-analysis pipeline'覺n varsay覺lan kontrol noktas覺 distilbert-base-uncased-finetuned-sst-2-english olduundan (model kart覺n覺 burada g繹rebilirsiniz), aa覺dakileri 癟al覺t覺r覺yoruz:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:31:28.275775Z","iopub.execute_input":"2024-08-01T20:31:28.276176Z","iopub.status.idle":"2024-08-01T20:31:36.530309Z","shell.execute_reply.started":"2024-08-01T20:31:28.276146Z","shell.execute_reply":"2024-08-01T20:31:36.529040Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ba6d854f21478785b119397586f66b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8005f0ae8e4b7d8494e0ebc4922339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd0d254a054c44c7965d42f0c3aa48fd"}},"metadata":{}}]},{"cell_type":"markdown","source":"Bir kez tokenizer'a sahip olduumuzda, c羹mlelerimizi dorudan ona aktarabiliriz ve modelimize beslemeye haz覺r bir s繹zl羹k geri al覺r覺z! Geriye kalan tek ey, input ID'lerinin listesini tens繹rlere d繹n羹t羹rmektir.\n\nArka u癟 olarak hangi ML 癟er癟evesinin kullan覺ld覺覺 konusunda endielenmenize gerek kalmadan  Transformers'覺 kullanabilirsiniz; PyTorch veya TensorFlow veya baz覺 modeller i癟in Flax olabilir. Ancak, Transformer modelleri girdi olarak yaln覺zca tens繹rleri kabul eder. Tens繹rleri ilk kez duyuyorsan覺z, bunlar覺 NumPy dizileri olarak d羹羹nebilirsiniz. Bir NumPy dizisi skaler (0D), vekt繹r (1D), matris (2D) veya daha fazla boyuta sahip olabilir. Etkili bir ekilde bir tens繹rd羹r; dier ML 癟er癟evelerinin tens繹rleri benzer ekilde davran覺r ve genellikle NumPy dizileri gibi 繹rneklenmesi kolayd覺r.\n\nGeri almak istediimiz tens繹r t羹r羹n羹 (PyTorch, TensorFlow veya d羹z NumPy) belirtmek i癟in return_tensors arg羹man覺n覺 kullan覺r覺z:","metadata":{}},{"cell_type":"code","source":"raw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:38:25.280346Z","iopub.execute_input":"2024-08-01T20:38:25.282009Z","iopub.status.idle":"2024-08-01T20:38:25.337798Z","shell.execute_reply.started":"2024-08-01T20:38:25.281961Z","shell.execute_reply":"2024-08-01T20:38:25.336337Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Hen羹z dolgu ve kesme konusunda endielenmeyin; bunlar覺 daha sonra a癟覺klayaca覺z. Burada hat覺rlanmas覺 gereken ana eyler, bir c羹mle veya bir c羹mle listesi ge癟ebileceiniz ve geri almak istediiniz tens繹rlerin t羹r羹n羹 belirtebileceinizdir (herhangi bir t羹r ge癟ilmezse, sonu癟 olarak bir liste listesi al覺rs覺n覺z).\n\n襤te sonu癟lar覺n PyTorch tens繹rleri olarak nas覺l g繹r羹nd羹羹:","metadata":{}},{"cell_type":"markdown","source":"覺kt覺n覺n kendisi iki anahtar i癟eren bir s繹zl羹kt羹r, input_ids ve attention_mask. input_ids, her c羹mledeki belirte癟lerin benzersiz tan覺mlay覺c覺lar覺 olan iki tamsay覺 sat覺r覺 (her c羹mle i癟in bir tane) i癟erir. attention_mask'覺n ne olduunu bu b繹l羹m羹n ilerleyen k覺s覺mlar覺nda a癟覺klayaca覺z.","metadata":{}},{"cell_type":"markdown","source":"## Going through the model\n\nnceden eitilmi modelimizi tokenizer ile yapt覺覺m覺z gibi indirebiliriz. Transformers, ayn覺 zamanda bir **from_pretrained()** y繹ntemine sahip olan bir **AutoModel** s覺n覺f覺 salar:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:41:45.853470Z","iopub.execute_input":"2024-08-01T20:41:45.854720Z","iopub.status.idle":"2024-08-01T20:41:50.491878Z","shell.execute_reply.started":"2024-08-01T20:41:45.854680Z","shell.execute_reply":"2024-08-01T20:41:50.490628Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfac9765575a4990ba700fba8041817a"}},"metadata":{}}]},{"cell_type":"markdown","source":"Bu kod par癟ac覺覺nda, daha 繹nce pipeline'覺m覺zda kulland覺覺m覺z ayn覺 kontrol noktas覺n覺 indirdik (asl覺nda zaten 繹nbellee al覺nm覺 olmal覺yd覺) ve onunla bir model oluturduk.\n\nBu mimari yaln覺zca temel Transformer mod羹l羹n羹 i癟erir: baz覺 girdiler verildiinde, 繹zellikler olarak da bilinen gizli durumlar olarak adland覺raca覺m覺z 癟覺kt覺lar覺 verir. Her model girdisi i癟in, Transformer modeli taraf覺ndan bu girdinin balamsal olarak anla覺lmas覺n覺 temsil eden y羹ksek boyutlu bir vekt繹r alaca覺z.\n\nBu bir anlam ifade etmiyorsa endielenmeyin. Hepsini daha sonra a癟覺klayaca覺z.\n\nBu gizli durumlar kendi balar覺na faydal覺 olsalar da, genellikle modelin kafa olarak bilinen baka bir b繹l羹m羹ne girdilerdir. B繹l羹m 1'de, farkl覺 g繹revler ayn覺 mimariyle ger癟ekletirilebilirdi, ancak bu g繹revlerin her biri kendisiyle ilikili farkl覺 bir kafaya sahip olacakt覺r.\n\nY羹ksek boyutlu bir vekt繹r m羹?\n\nTransformat繹r mod羹l羹n羹n vekt繹r 癟覺kt覺s覺 genellikle b羹y羹kt羹r. Genellikle 羹癟 boyuta sahiptir:\n\n- Batch size: Bir seferde ilenen dizi say覺s覺 (繹rneimizde 2).\n- Dizi uzunluu: Dizinin say覺sal temsilinin uzunluu (繹rneimizde 16).\n- Gizli boyut: Her bir model girdisinin vekt繹r boyutu.\n\nSon deer nedeniyle \"y羹ksek boyutlu\" olduu s繹ylenir. Gizli boyut 癟ok b羹y羹k olabilir (daha k羹癟羹k modeller i癟in 768 yayg覺nd覺r ve daha b羹y羹k modellerde bu 3072 veya daha fazlas覺na ulaabilir).\n\nn ilemden ge癟irdiimiz girdileri modelimize beslersek bunu g繹rebiliriz:","metadata":{}},{"cell_type":"code","source":"outputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:44:47.474540Z","iopub.execute_input":"2024-08-01T20:44:47.475231Z","iopub.status.idle":"2024-08-01T20:44:48.475144Z","shell.execute_reply.started":"2024-08-01T20:44:47.475188Z","shell.execute_reply":"2024-08-01T20:44:48.473938Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([2, 16, 768])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Transformers modellerinin 癟覺kt覺lar覺n覺n **namedtuple**s veya s繹zl羹kler gibi davrand覺覺n覺 unutmay覺n. elere niteliklere g繹re (bizim yapt覺覺m覺z gibi) veya anahtara g繹re (outputs[\"last_hidden_state\"]), hatta arad覺覺n覺z eyin tam olarak nerede olduunu biliyorsan覺z dizine g繹re (outputs[0]) eriebilirsiniz.","metadata":{}},{"cell_type":"markdown","source":"## Model heads: Making sense out of numbers\n\nModel kafalar覺, y羹ksek boyutlu gizli durum vekt繹r羹n羹 girdi olarak al覺r ve bunlar覺 farkl覺 bir boyuta yans覺t覺r. Genellikle bir veya birka癟 dorusal katmandan oluurlar:\n\n![image9](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg)\n\nTransformat繹r modelinin 癟覺kt覺s覺 ilenmek 羹zere dorudan model kafas覺na g繹nderilir.\n\nBu diyagramda model, g繹mme katman覺 ve sonraki katmanlar taraf覺ndan temsil edilmektedir. G繹mme katman覺, tokenize edilmi girdideki her bir girdi kimliini ilikili token覺 temsil eden bir vekt繹re d繹n羹t羹r羹r. Sonraki katmanlar, c羹mlelerin nihai temsilini 羹retmek i癟in dikkat mekanizmas覺n覺 kullanarak bu vekt繹rleri manip羹le eder.\n\nTransformers'da bir癟ok farkl覺 mimari mevcuttur ve her biri belirli bir g繹revin 羹stesinden gelmek 羹zere tasarlanm覺t覺r. 襤te kapsaml覺 olmayan bir liste:\n\n- *Model (retrieve the hidden states)\n- *ForCausalLM\n- *ForMaskedLM\n- *ForMultipleChoice\n- *ForQuestionAnswering\n- *ForSequenceClassification\n- *ForTokenClassification\n- and others\n\nrneimiz i癟in, dizi s覺n覺fland覺rma bal覺覺na sahip bir modele ihtiyac覺m覺z olacak (c羹mleleri pozitif veya negatif olarak s覺n覺fland覺rabilmek i癟in). Bu y羹zden, asl覺nda AutoModel s覺n覺f覺n覺 deil, AutoModelForSequenceClassification s覺n覺f覺n覺 kullanaca覺z:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs = model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:51:44.931444Z","iopub.execute_input":"2024-08-01T20:51:44.931947Z","iopub.status.idle":"2024-08-01T20:51:45.445732Z","shell.execute_reply.started":"2024-08-01T20:51:44.931913Z","shell.execute_reply":"2024-08-01T20:51:45.444481Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"imdi 癟覺kt覺lar覺m覺z覺n ekline bakarsak, boyutluluk 癟ok daha d羹羹k olacakt覺r: model kafas覺, daha 繹nce g繹rd羹羹m羹z y羹ksek boyutlu vekt繹rleri girdi olarak al覺r ve iki deer i癟eren vekt繹rleri (etiket ba覺na bir tane) 癟覺kt覺 olarak verir:","metadata":{}},{"cell_type":"code","source":"print(outputs.logits.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:00:34.841407Z","iopub.execute_input":"2024-08-01T21:00:34.842290Z","iopub.status.idle":"2024-08-01T21:00:34.848129Z","shell.execute_reply.started":"2024-08-01T21:00:34.842253Z","shell.execute_reply":"2024-08-01T21:00:34.846859Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"torch.Size([2, 2])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Elimizde sadece iki c羹mle ve iki etiket olduu i癟in modelimizden elde ettiimiz sonu癟 2 x 2 eklindedir.","metadata":{}},{"cell_type":"markdown","source":"## Postprocessing the output\n\nModelimizden 癟覺kt覺 olarak elde ettiimiz deerler kendi balar覺na bir anlam ifade etmeyebilir. Bir g繹z atal覺m:","metadata":{}},{"cell_type":"code","source":"print(outputs.logits)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:01:22.690870Z","iopub.execute_input":"2024-08-01T21:01:22.691969Z","iopub.status.idle":"2024-08-01T21:01:22.718292Z","shell.execute_reply.started":"2024-08-01T21:01:22.691930Z","shell.execute_reply":"2024-08-01T21:01:22.717121Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor([[-1.5607,  1.6123],\n        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Modelimiz ilk c羹mle i癟in [-1.5607, 1.6123] ve ikinci c羹mle i癟in [ 4.1692, -3.3464] tahmininde bulunmutur. Bunlar olas覺l覺k deil logittir, modelin son katman覺 taraf覺ndan 癟覺kar覺lan ham, normalize edilmemi puanlard覺r. Olas覺l覺klara d繹n羹t羹r羹lmeleri i癟in bir SoftMax katman覺ndan ge癟meleri gerekir (t羹m  Transformers modelleri logit 癟覺kt覺s覺 verir, 癟羹nk羹 eitim i癟in kay覺p fonksiyonu genellikle SoftMax gibi son aktivasyon fonksiyonunu 癟apraz entropi gibi ger癟ek kay覺p fonksiyonu ile birletirir):","metadata":{}},{"cell_type":"code","source":"import torch\n\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:02:24.003963Z","iopub.execute_input":"2024-08-01T21:02:24.004383Z","iopub.status.idle":"2024-08-01T21:02:24.012532Z","shell.execute_reply.started":"2024-08-01T21:02:24.004354Z","shell.execute_reply":"2024-08-01T21:02:24.010986Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[4.0195e-02, 9.5980e-01],\n        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"imdi modelin ilk c羹mle i癟in [0.0402, 0.9598] ve ikinci c羹mle i癟in [0.9995, 0.0005] tahmininde bulunduunu g繹rebiliriz. Bunlar tan覺nabilir olas覺l覺k puanlar覺d覺r.\n\nHer bir pozisyona kar覺l覺k gelen etiketleri elde etmek i癟in model yap覺land覺rmas覺n覺n id2label niteliini inceleyebiliriz (bu konuda daha fazla bilgi bir sonraki b繹l羹mde):","metadata":{}},{"cell_type":"code","source":"model.config.id2label","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:04:12.003005Z","iopub.execute_input":"2024-08-01T21:04:12.004164Z","iopub.status.idle":"2024-08-01T21:04:12.012036Z","shell.execute_reply.started":"2024-08-01T21:04:12.004116Z","shell.execute_reply":"2024-08-01T21:04:12.010821Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}]},{"cell_type":"markdown","source":"imdi modelin aa覺dakileri 繹ng繹rd羹羹 sonucuna varabiliriz:\n\n- 襤lk c羹mle: NEGAT襤F: 0,0402, POZ襤T襤F: 0,9598 \n- 襤kinci c羹mle: NEGAT襤F: 0.9995, POZ襤T襤F: 0.0005 \n\nPipeline'n覺n 羹癟 ad覺m覺n覺 baar覺yla yeniden 羹rettik: belirte癟lerle 繹n ileme, girdileri aktarma","metadata":{}}]}