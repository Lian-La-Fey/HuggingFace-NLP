{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nBölüm 1'de gördüğünüz gibi, Transformatör modelleri genellikle çok büyüktür. Milyonlarca ila on milyarlarca parametre içeren bu modelleri eğitmek ve dağıtmak karmaşık bir iştir. Ayrıca, neredeyse her gün yeni modellerin piyasaya sürülmesi ve her birinin kendi uygulamasına sahip olması nedeniyle, hepsini denemek kolay bir iş değildir.\n\nTransformers kütüphanesi bu sorunu çözmek için oluşturulmuştur. Amacı, herhangi bir Transformer modelinin yüklenebileceği, eğitilebileceği ve kaydedilebileceği tek bir API sağlamaktır. Kütüphanenin ana özellikleri şunlardır:\n\n- Kullanım kolaylığı: Son teknoloji ürünü bir NLP modelinin indirilmesi, yüklenmesi ve çıkarım için kullanılması sadece iki satır kodla yapılabilir. \n\n- Esneklik: Özünde, tüm modeller basit PyTorch nn.Module veya TensorFlow tf.keras.Model sınıflarıdır ve ilgili makine öğrenimi (ML) çerçevelerindeki diğer modeller gibi ele alınabilir. \n\n- Basitlik: Kütüphane genelinde neredeyse hiç soyutlama yapılmamıştır. \"Hepsi tek bir dosyada\" temel bir kavramdır: bir modelin ileri geçişi tamamen tek bir dosyada tanımlanır, böylece kodun kendisi anlaşılabilir ve hacklenebilir. \n\nBu son özellik Transformers'ı diğer ML kütüphanelerinden oldukça farklı kılar. Modeller, dosyalar arasında paylaşılan modüller üzerine inşa edilmemiştir; bunun yerine, her modelin kendi katmanları vardır. Bu, modelleri daha ulaşılabilir ve anlaşılabilir kılmanın yanı sıra, bir model üzerinde diğerlerini etkilemeden kolayca deney yapabilmenizi sağlar.\n\nBu bölüm, Bölüm 1'de tanıtılan pipeline() fonksiyonunu kopyalamak için bir model ve bir tokenizer'ı birlikte kullandığımız uçtan uca bir örnekle baş\"\"layacaktır. Ardından, model API'sini tartışacağız: model ve yapılandırma sınıflarına dalacağız ve bir modelin nasıl yükleneceğini ve tahminlerin çıktısını almak için sayısal girdileri nasıl işlediğini göstereceğiz.\n\nArdından pipeline() fonksiyonunun diğer ana bileşeni olan tokenizer API'sine bakacağız. Tokenizer'lar, sinir ağı için metinden sayısal girdilere dönüştürme ve gerektiğinde metne geri dönüştürme işlemlerini gerçekleştirerek ilk ve son işleme adımlarıyla ilgilenir. Son olarak, birden fazla cümlenin bir model aracılığıyla hazırlanmış bir yığın halinde nasıl gönderileceğini göstereceğiz ve ardından üst düzey tokenizer() işlevine daha yakından bakarak her şeyi tamamlayacağız.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Behind the pipeline\n\nBölüm 1'de aşağıdaki kodu çalıştırdığımızda perde arkasında neler olduğuna bir göz atarak tam bir örnekle başlayalım:","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\n    [\n        \"I've been waiting for a HuggingFace course my whole life.\",\n        \"I hate this so much!\",\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T19:11:32.014763Z","iopub.execute_input":"2024-08-01T19:11:32.015240Z","iopub.status.idle":"2024-08-01T19:12:05.078432Z","shell.execute_reply.started":"2024-08-01T19:11:32.015206Z","shell.execute_reply":"2024-08-01T19:12:05.076241Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-01 19:11:42.230271: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-01 19:11:42.230460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-01 19:11:42.441068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88248b8d10184a36a8cb955fe7541739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5925c64d4b7c4f65b9b8667127415639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5c36663c4041d296a9ee1403aa8e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a180104541544c799d3e2113c6d557c"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Bölüm 1'de gördüğümüz gibi, bu pipeline üç adımı bir araya getirir: ön işleme, girdileri modelden geçirme ve son işleme:\n\n![image1](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg)","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing with a tokenizer\n\nDiğer sinir ağları gibi Transformer modelleri de ham metni doğrudan işleyemez, bu nedenle boru hattımızın ilk adımı metin girdilerini modelin anlamlandırabileceği sayılara dönüştürmektir. Bunu yapmak için aşağıdakilerden sorumlu olacak bir tokenizer kullanıyoruz:\n\n- Girdiyi token adı verilen kelimelere, alt kelimelere veya sembollere (noktalama işaretleri gibi) bölme\n- Her bir tokeni bir tamsayıya eşleme\n- Modele faydalı olabilecek ek girdilerin eklenmesi\n\n\nTüm bu ön işlemlerin modelin ön eğitime tabi tutulmasıyla tamamen aynı şekilde yapılması gerekir, bu nedenle öncelikle bu bilgileri Model Hub'ından indirmemiz gerekir. Bunu yapmak için AutoTokenizer sınıfını ve onun from_pretrained() yöntemini kullanırız. Modelimizin kontrol noktası adını kullanarak, modelin tokenizer'ıyla ilişkili verileri otomatik olarak getirecek ve önbelleğe alacaktır (böylece yalnızca aşağıdaki kodu ilk kez çalıştırdığınızda indirilir).\n\nSentiment-analysis pipeline'ın varsayılan kontrol noktası distilbert-base-uncased-finetuned-sst-2-english olduğundan (model kartını burada görebilirsiniz), aşağıdakileri çalıştırıyoruz:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:31:28.275775Z","iopub.execute_input":"2024-08-01T20:31:28.276176Z","iopub.status.idle":"2024-08-01T20:31:36.530309Z","shell.execute_reply.started":"2024-08-01T20:31:28.276146Z","shell.execute_reply":"2024-08-01T20:31:36.529040Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ba6d854f21478785b119397586f66b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8005f0ae8e4b7d8494e0ebc4922339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd0d254a054c44c7965d42f0c3aa48fd"}},"metadata":{}}]},{"cell_type":"markdown","source":"Bir kez tokenizer'a sahip olduğumuzda, cümlelerimizi doğrudan ona aktarabiliriz ve modelimize beslemeye hazır bir sözlük geri alırız! Geriye kalan tek şey, input ID'lerinin listesini tensörlere dönüştürmektir.\n\nArka uç olarak hangi ML çerçevesinin kullanıldığı konusunda endişelenmenize gerek kalmadan  Transformers'ı kullanabilirsiniz; PyTorch veya TensorFlow veya bazı modeller için Flax olabilir. Ancak, Transformer modelleri girdi olarak yalnızca tensörleri kabul eder. Tensörleri ilk kez duyuyorsanız, bunları NumPy dizileri olarak düşünebilirsiniz. Bir NumPy dizisi skaler (0D), vektör (1D), matris (2D) veya daha fazla boyuta sahip olabilir. Etkili bir şekilde bir tensördür; diğer ML çerçevelerinin tensörleri benzer şekilde davranır ve genellikle NumPy dizileri gibi örneklenmesi kolaydır.\n\nGeri almak istediğimiz tensör türünü (PyTorch, TensorFlow veya düz NumPy) belirtmek için return_tensors argümanını kullanırız:","metadata":{}},{"cell_type":"code","source":"raw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:38:25.280346Z","iopub.execute_input":"2024-08-01T20:38:25.282009Z","iopub.status.idle":"2024-08-01T20:38:25.337798Z","shell.execute_reply.started":"2024-08-01T20:38:25.281961Z","shell.execute_reply":"2024-08-01T20:38:25.336337Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Henüz dolgu ve kesme konusunda endişelenmeyin; bunları daha sonra açıklayacağız. Burada hatırlanması gereken ana şeyler, bir cümle veya bir cümle listesi geçebileceğiniz ve geri almak istediğiniz tensörlerin türünü belirtebileceğinizdir (herhangi bir tür geçilmezse, sonuç olarak bir liste listesi alırsınız).\n\nİşte sonuçların PyTorch tensörleri olarak nasıl göründüğü:","metadata":{}},{"cell_type":"markdown","source":"Çıktının kendisi iki anahtar içeren bir sözlüktür, input_ids ve attention_mask. input_ids, her cümledeki belirteçlerin benzersiz tanımlayıcıları olan iki tamsayı satırı (her cümle için bir tane) içerir. attention_mask'ın ne olduğunu bu bölümün ilerleyen kısımlarında açıklayacağız.","metadata":{}},{"cell_type":"markdown","source":"## Going through the model\n\nÖnceden eğitilmiş modelimizi tokenizer ile yaptığımız gibi indirebiliriz. Transformers, aynı zamanda bir **from_pretrained()** yöntemine sahip olan bir **AutoModel** sınıfı sağlar:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:41:45.853470Z","iopub.execute_input":"2024-08-01T20:41:45.854720Z","iopub.status.idle":"2024-08-01T20:41:50.491878Z","shell.execute_reply.started":"2024-08-01T20:41:45.854680Z","shell.execute_reply":"2024-08-01T20:41:50.490628Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfac9765575a4990ba700fba8041817a"}},"metadata":{}}]},{"cell_type":"markdown","source":"Bu kod parçacığında, daha önce pipeline'ımızda kullandığımız aynı kontrol noktasını indirdik (aslında zaten önbelleğe alınmış olmalıydı) ve onunla bir model oluşturduk.\n\nBu mimari yalnızca temel Transformer modülünü içerir: bazı girdiler verildiğinde, özellikler olarak da bilinen gizli durumlar olarak adlandıracağımız çıktıları verir. Her model girdisi için, Transformer modeli tarafından bu girdinin bağlamsal olarak anlaşılmasını temsil eden yüksek boyutlu bir vektör alacağız.\n\nBu bir anlam ifade etmiyorsa endişelenmeyin. Hepsini daha sonra açıklayacağız.\n\nBu gizli durumlar kendi başlarına faydalı olsalar da, genellikle modelin kafa olarak bilinen başka bir bölümüne girdilerdir. Bölüm 1'de, farklı görevler aynı mimariyle gerçekleştirilebilirdi, ancak bu görevlerin her biri kendisiyle ilişkili farklı bir kafaya sahip olacaktır.\n\nYüksek boyutlu bir vektör mü?\n\nTransformatör modülünün vektör çıktısı genellikle büyüktür. Genellikle üç boyuta sahiptir:\n\n- Batch size: Bir seferde işlenen dizi sayısı (örneğimizde 2).\n- Dizi uzunluğu: Dizinin sayısal temsilinin uzunluğu (örneğimizde 16).\n- Gizli boyut: Her bir model girdisinin vektör boyutu.\n\nSon değer nedeniyle \"yüksek boyutlu\" olduğu söylenir. Gizli boyut çok büyük olabilir (daha küçük modeller için 768 yaygındır ve daha büyük modellerde bu 3072 veya daha fazlasına ulaşabilir).\n\nÖn işlemden geçirdiğimiz girdileri modelimize beslersek bunu görebiliriz:","metadata":{}},{"cell_type":"code","source":"outputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:44:47.474540Z","iopub.execute_input":"2024-08-01T20:44:47.475231Z","iopub.status.idle":"2024-08-01T20:44:48.475144Z","shell.execute_reply.started":"2024-08-01T20:44:47.475188Z","shell.execute_reply":"2024-08-01T20:44:48.473938Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([2, 16, 768])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Transformers modellerinin çıktılarının **namedtuple**s veya sözlükler gibi davrandığını unutmayın. Öğelere niteliklere göre (bizim yaptığımız gibi) veya anahtara göre (outputs[\"last_hidden_state\"]), hatta aradığınız şeyin tam olarak nerede olduğunu biliyorsanız dizine göre (outputs[0]) erişebilirsiniz.","metadata":{}},{"cell_type":"markdown","source":"## Model heads: Making sense out of numbers\n\nModel kafaları, yüksek boyutlu gizli durum vektörünü girdi olarak alır ve bunları farklı bir boyuta yansıtır. Genellikle bir veya birkaç doğrusal katmandan oluşurlar:\n\n![image9](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg)\n\nTransformatör modelinin çıktısı işlenmek üzere doğrudan model kafasına gönderilir.\n\nBu diyagramda model, gömme katmanı ve sonraki katmanlar tarafından temsil edilmektedir. Gömme katmanı, tokenize edilmiş girdideki her bir girdi kimliğini ilişkili tokenı temsil eden bir vektöre dönüştürür. Sonraki katmanlar, cümlelerin nihai temsilini üretmek için dikkat mekanizmasını kullanarak bu vektörleri manipüle eder.\n\nTransformers'da birçok farklı mimari mevcuttur ve her biri belirli bir görevin üstesinden gelmek üzere tasarlanmıştır. İşte kapsamlı olmayan bir liste:\n\n- *Model (retrieve the hidden states)\n- *ForCausalLM\n- *ForMaskedLM\n- *ForMultipleChoice\n- *ForQuestionAnswering\n- *ForSequenceClassification\n- *ForTokenClassification\n- and others\n\nÖrneğimiz için, dizi sınıflandırma başlığına sahip bir modele ihtiyacımız olacak (cümleleri pozitif veya negatif olarak sınıflandırabilmek için). Bu yüzden, aslında AutoModel sınıfını değil, AutoModelForSequenceClassification sınıfını kullanacağız:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs = model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:51:44.931444Z","iopub.execute_input":"2024-08-01T20:51:44.931947Z","iopub.status.idle":"2024-08-01T20:51:45.445732Z","shell.execute_reply.started":"2024-08-01T20:51:44.931913Z","shell.execute_reply":"2024-08-01T20:51:45.444481Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Şimdi çıktılarımızın şekline bakarsak, boyutluluk çok daha düşük olacaktır: model kafası, daha önce gördüğümüz yüksek boyutlu vektörleri girdi olarak alır ve iki değer içeren vektörleri (etiket başına bir tane) çıktı olarak verir:","metadata":{}},{"cell_type":"code","source":"print(outputs.logits.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:00:34.841407Z","iopub.execute_input":"2024-08-01T21:00:34.842290Z","iopub.status.idle":"2024-08-01T21:00:34.848129Z","shell.execute_reply.started":"2024-08-01T21:00:34.842253Z","shell.execute_reply":"2024-08-01T21:00:34.846859Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"torch.Size([2, 2])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Elimizde sadece iki cümle ve iki etiket olduğu için modelimizden elde ettiğimiz sonuç 2 x 2 şeklindedir.","metadata":{}},{"cell_type":"markdown","source":"## Postprocessing the output\n\nModelimizden çıktı olarak elde ettiğimiz değerler kendi başlarına bir anlam ifade etmeyebilir. Bir göz atalım:","metadata":{}},{"cell_type":"code","source":"print(outputs.logits)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:01:22.690870Z","iopub.execute_input":"2024-08-01T21:01:22.691969Z","iopub.status.idle":"2024-08-01T21:01:22.718292Z","shell.execute_reply.started":"2024-08-01T21:01:22.691930Z","shell.execute_reply":"2024-08-01T21:01:22.717121Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor([[-1.5607,  1.6123],\n        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Modelimiz ilk cümle için [-1.5607, 1.6123] ve ikinci cümle için [ 4.1692, -3.3464] tahmininde bulunmuştur. Bunlar olasılık değil logittir, modelin son katmanı tarafından çıkarılan ham, normalize edilmemiş puanlardır. Olasılıklara dönüştürülmeleri için bir SoftMax katmanından geçmeleri gerekir (tüm 🤗 Transformers modelleri logit çıktısı verir, çünkü eğitim için kayıp fonksiyonu genellikle SoftMax gibi son aktivasyon fonksiyonunu çapraz entropi gibi gerçek kayıp fonksiyonu ile birleştirir):","metadata":{}},{"cell_type":"code","source":"import torch\n\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:02:24.003963Z","iopub.execute_input":"2024-08-01T21:02:24.004383Z","iopub.status.idle":"2024-08-01T21:02:24.012532Z","shell.execute_reply.started":"2024-08-01T21:02:24.004354Z","shell.execute_reply":"2024-08-01T21:02:24.010986Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[4.0195e-02, 9.5980e-01],\n        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Şimdi modelin ilk cümle için [0.0402, 0.9598] ve ikinci cümle için [0.9995, 0.0005] tahmininde bulunduğunu görebiliriz. Bunlar tanınabilir olasılık puanlarıdır.\n\nHer bir pozisyona karşılık gelen etiketleri elde etmek için model yapılandırmasının id2label niteliğini inceleyebiliriz (bu konuda daha fazla bilgi bir sonraki bölümde):","metadata":{}},{"cell_type":"code","source":"model.config.id2label","metadata":{"execution":{"iopub.status.busy":"2024-08-01T21:04:12.003005Z","iopub.execute_input":"2024-08-01T21:04:12.004164Z","iopub.status.idle":"2024-08-01T21:04:12.012036Z","shell.execute_reply.started":"2024-08-01T21:04:12.004116Z","shell.execute_reply":"2024-08-01T21:04:12.010821Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}]},{"cell_type":"markdown","source":"Şimdi modelin aşağıdakileri öngördüğü sonucuna varabiliriz:\n\n- İlk cümle: NEGATİF: 0,0402, POZİTİF: 0,9598 \n- İkinci cümle: NEGATİF: 0.9995, POZİTİF: 0.0005 \n\nPipeline'nın üç adımını başarıyla yeniden ürettik: belirteçlerle ön işleme, girdileri aktarma","metadata":{}}]}